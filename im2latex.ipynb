{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# - Restrict to short equations < 50\n",
    "# - Crop more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To plot learning curve graph\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for pretty print\n",
    "from pprint import pprint\n",
    "\n",
    "# for tokenizer\n",
    "import re\n",
    "\n",
    "# for word counter in vocabulary dictionary\n",
    "from collections import Counter\n",
    "\n",
    "# for checkpoint paths\n",
    "import os\n",
    "\n",
    "# for fancy progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# for output_projection\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# for initial attention (not required ver1.2+)\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'eq_images/cropped'\n",
    "files = list(set(os.listdir(os.path.join(path,'images'))) - set(['.DS_Store']))\n",
    "imbeds = list(set(os.listdir('imbeddings2')) - set(['.DS_Store']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(path, '/Users/user/eecs/Deep_Learning/im2latex/im2latex_train.lst')) as fl:\n",
    "    ids = fl.readlines()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_name_pairs = [(id_[:-1].split())[:-1] for id_ in ids]\n",
    "imbed2id = dict([(name+'.npy',int(id_)) for id_, name in id_name_pairs])\n",
    "im2id = dict([(name+'.png',int(id_)) for id_, name in id_name_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/Users/user/eecs/Deep_Learning/im2latex/im2latex_formulas.lst','rb') as f:\n",
    "    eqs = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eqs = np.array(eqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_eqs = eqs[list(map(imbed2id.get,imbeds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice = np.random.randint(0,len(files))\n",
    "choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11a744eb8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAB9CAYAAAA/bGDPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11769a358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(plt.imread(os.path.join(path,'images',imbeds[choice][:-4]+'.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'M_W^2 = {g^2_2 v^2 \\\\over 4} \\\\qquad M_Z^2 =  {g^2_2 v^2 \\\\over 4 \\\\cos^2 \\\\theta_W}\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eqs[choice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'y']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    tokenizer_ = RegexpTokenizer('\\\\\\\\[A-Za-z]+|\\\\\\\\\\S|\\[A-Za-z]+|\\d|\\S')\n",
    "    if type(sentence) is not str:\n",
    "        sentence = sentence.decode()\n",
    "    return tokenizer_.tokenize(sentence)\n",
    "\n",
    "# Example\n",
    "tokenizer(\"xy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'y', '+', 'z']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "def replace_label(sentence):\n",
    "    if type(sentence) is not str:\n",
    "        sentence = sentence.decode()\n",
    "    return re.sub(string=sentence,\n",
    "                      pattern=r'\\\\label\\{[^\\}]+\\}',\n",
    "                      repl='')\n",
    "\n",
    "\n",
    "def tokenizer_no_label(sentence):\n",
    "    tokenizer_ = RegexpTokenizer('\\\\\\\\[A-Za-z]+|\\\\\\\\\\S|\\[A-Za-z]+|\\d|\\S')\n",
    "    sentence = replace_label(sentence)\n",
    "    return tokenizer_.tokenize(sentence)\n",
    "\n",
    "# Example\n",
    "tokenizer_no_label(\"xy \\\\label{this is a label} + z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_length(eq):\n",
    "    try:\n",
    "        return len(tokenizer_no_label(eq))\n",
    "    except:\n",
    "        return np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgs = [np.load(os.path.join('imbeddings2',imbed)) for imbed in imbeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40276"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be lazy and retain a multiple of 128 for quick batching without iterators and generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40192"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ex= 128*(len(imgs)//128)\n",
    "num_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_tr, x_val, y_tr, y_val = train_test_split(imgs, train_eqs, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_batches = [y_tr[i:i+128] for i in range(0,len(y_tr),128)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_batches = [x_tr[i:i+128] for i in range(0,len(x_tr),128)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_target_sentences = []\n",
    "for target_batch in target_batches:\n",
    "    all_target_sentences.extend(target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'!': 184,\n",
      "  '\"': 320,\n",
      "  '$': 174,\n",
      "  '%': 138,\n",
      "  '&': 148,\n",
      "  \"'\": 62,\n",
      "  '(': 7,\n",
      "  ')': 6,\n",
      "  '*': 118,\n",
      "  '+': 13,\n",
      "  ',': 10,\n",
      "  '-': 12,\n",
      "  '.': 15,\n",
      "  '/': 86,\n",
      "  '0': 16,\n",
      "  '1': 11,\n",
      "  '2': 9,\n",
      "  '3': 40,\n",
      "  '4': 45,\n",
      "  '5': 116,\n",
      "  '6': 127,\n",
      "  '7': 169,\n",
      "  '8': 135,\n",
      "  '9': 171,\n",
      "  ':': 147,\n",
      "  ';': 136,\n",
      "  '<': 141,\n",
      "  '=': 8,\n",
      "  '>': 137,\n",
      "  '?': 360,\n",
      "  '@': 440,\n",
      "  'A': 27,\n",
      "  'B': 68,\n",
      "  'C': 83,\n",
      "  'D': 65,\n",
      "  'E': 85,\n",
      "  'F': 69,\n",
      "  'G': 89,\n",
      "  'H': 76,\n",
      "  'I': 94,\n",
      "  'J': 98,\n",
      "  'K': 107,\n",
      "  'L': 64,\n",
      "  'M': 60,\n",
      "  'N': 61,\n",
      "  'O': 143,\n",
      "  'P': 82,\n",
      "  'Q': 97,\n",
      "  'R': 59,\n",
      "  'S': 52,\n",
      "  'T': 58,\n",
      "  'U': 111,\n",
      "  'V': 81,\n",
      "  'W': 105,\n",
      "  'X': 90,\n",
      "  'Y': 153,\n",
      "  'Z': 113,\n",
      "  '[': 48,\n",
      "  '\\\\': 33,\n",
      "  '\\\\!': 155,\n",
      "  '\\\\#': 268,\n",
      "  '\\\\$': 399,\n",
      "  '\\\\%': 438,\n",
      "  '\\\\&': 446,\n",
      "  '\\\\,': 20,\n",
      "  '\\\\-': 400,\n",
      "  '\\\\/': 322,\n",
      "  '\\\\:': 158,\n",
      "  '\\\\;': 31,\n",
      "  '\\\\>': 214,\n",
      "  '\\\\AA': 444,\n",
      "  '\\\\Big': 192,\n",
      "  '\\\\Bigg': 323,\n",
      "  '\\\\Biggl': 316,\n",
      "  '\\\\Biggr': 313,\n",
      "  '\\\\Bigl': 225,\n",
      "  '\\\\Bigr': 224,\n",
      "  '\\\\Box': 470,\n",
      "  '\\\\Delta': 112,\n",
      "  '\\\\Gamma': 109,\n",
      "  '\\\\Huge': 473,\n",
      "  '\\\\Im': 283,\n",
      "  '\\\\L': 264,\n",
      "  '\\\\LARGE': 458,\n",
      "  '\\\\Lambda': 108,\n",
      "  '\\\\Large': 354,\n",
      "  '\\\\Leftrightarrow': 297,\n",
      "  '\\\\Longleftarrow': 428,\n",
      "  '\\\\Longleftrightarrow': 296,\n",
      "  '\\\\Longrightarrow': 330,\n",
      "  '\\\\O': 287,\n",
      "  '\\\\Omega': 117,\n",
      "  '\\\\P': 321,\n",
      "  '\\\\Phi': 99,\n",
      "  '\\\\Pi': 162,\n",
      "  '\\\\Psi': 126,\n",
      "  '\\\\Re': 292,\n",
      "  '\\\\Rightarrow': 245,\n",
      "  '\\\\S': 295,\n",
      "  '\\\\SS': 406,\n",
      "  '\\\\Sigma': 144,\n",
      "  '\\\\Theta': 177,\n",
      "  '\\\\Upsilon': 259,\n",
      "  '\\\\Vert': 312,\n",
      "  '\\\\Xi': 217,\n",
      "  '\\\\\\\\': 168,\n",
      "  '\\\\^': 450,\n",
      "  '\\\\_': 374,\n",
      "  '\\\\acute': 347,\n",
      "  '\\\\aleph': 364,\n",
      "  '\\\\alpha': 30,\n",
      "  '\\\\amalg': 372,\n",
      "  '\\\\approx': 172,\n",
      "  '\\\\arccos': 391,\n",
      "  '\\\\arcsin': 389,\n",
      "  '\\\\arctan': 302,\n",
      "  '\\\\arg': 300,\n",
      "  '\\\\arrowvert': 432,\n",
      "  '\\\\ast': 182,\n",
      "  '\\\\atop': 327,\n",
      "  '\\\\atopwithdelims': 443,\n",
      "  '\\\\b': 401,\n",
      "  '\\\\backslash': 337,\n",
      "  '\\\\bar': 71,\n",
      "  '\\\\begin': 179,\n",
      "  '\\\\beta': 63,\n",
      "  '\\\\bf': 106,\n",
      "  '\\\\big': 206,\n",
      "  '\\\\bigcap': 380,\n",
      "  '\\\\bigcup': 309,\n",
      "  '\\\\bigg': 215,\n",
      "  '\\\\biggl': 223,\n",
      "  '\\\\biggm': 436,\n",
      "  '\\\\biggr': 227,\n",
      "  '\\\\bigl': 226,\n",
      "  '\\\\bigm': 394,\n",
      "  '\\\\bigoplus': 262,\n",
      "  '\\\\bigotimes': 307,\n",
      "  '\\\\bigr': 221,\n",
      "  '\\\\bigsqcup': 422,\n",
      "  '\\\\bigtriangledown': 392,\n",
      "  '\\\\bigtriangleup': 310,\n",
      "  '\\\\bigvee': 476,\n",
      "  '\\\\bigwedge': 435,\n",
      "  '\\\\bmod': 357,\n",
      "  '\\\\boldmath': 228,\n",
      "  '\\\\bot': 261,\n",
      "  '\\\\brack': 462,\n",
      "  '\\\\breve': 298,\n",
      "  '\\\\buildrel': 324,\n",
      "  '\\\\bullet': 276,\n",
      "  '\\\\c': 408,\n",
      "  '\\\\cal': 54,\n",
      "  '\\\\cap': 277,\n",
      "  '\\\\case': 468,\n",
      "  '\\\\cases': 333,\n",
      "  '\\\\cdot': 133,\n",
      "  '\\\\cdotp': 338,\n",
      "  '\\\\cdots': 164,\n",
      "  '\\\\check': 256,\n",
      "  '\\\\chi': 121,\n",
      "  '\\\\choose': 275,\n",
      "  '\\\\circ': 198,\n",
      "  '\\\\colon': 345,\n",
      "  '\\\\cong': 251,\n",
      "  '\\\\cos': 176,\n",
      "  '\\\\cosh': 200,\n",
      "  '\\\\cot': 279,\n",
      "  '\\\\coth': 289,\n",
      "  '\\\\cr': 185,\n",
      "  '\\\\csc': 474,\n",
      "  '\\\\cup': 269,\n",
      "  '\\\\d': 253,\n",
      "  '\\\\dag': 213,\n",
      "  '\\\\dagger': 151,\n",
      "  '\\\\ddag': 407,\n",
      "  '\\\\ddagger': 455,\n",
      "  '\\\\ddot': 209,\n",
      "  '\\\\def': 449,\n",
      "  '\\\\deg': 318,\n",
      "  '\\\\delta': 55,\n",
      "  '\\\\det': 201,\n",
      "  '\\\\diamond': 370,\n",
      "  '\\\\diamondsuit': 383,\n",
      "  '\\\\dicov': 433,\n",
      "  '\\\\dim': 308,\n",
      "  '\\\\displaystyle': 232,\n",
      "  '\\\\ditil': 467,\n",
      "  '\\\\do': 414,\n",
      "  '\\\\dot': 122,\n",
      "  '\\\\doteq': 304,\n",
      "  '\\\\dots': 193,\n",
      "  '\\\\downarrow': 288,\n",
      "  '\\\\ell': 159,\n",
      "  '\\\\emptyset': 341,\n",
      "  '\\\\end': 178,\n",
      "  '\\\\enskip': 355,\n",
      "  '\\\\enspace': 340,\n",
      "  '\\\\ensuremath': 424,\n",
      "  '\\\\epsilon': 92,\n",
      "  '\\\\eqno': 331,\n",
      "  '\\\\equiv': 119,\n",
      "  '\\\\eta': 100,\n",
      "  '\\\\exists': 418,\n",
      "  '\\\\exp': 152,\n",
      "  '\\\\fbox': 472,\n",
      "  '\\\\flat': 398,\n",
      "  '\\\\footnotemark': 445,\n",
      "  '\\\\footnotesize': 365,\n",
      "  '\\\\forall': 230,\n",
      "  '\\\\frac': 17,\n",
      "  '\\\\ft': 429,\n",
      "  '\\\\gamma': 72,\n",
      "  '\\\\ge': 216,\n",
      "  '\\\\geq': 195,\n",
      "  '\\\\gg': 249,\n",
      "  '\\\\grave': 420,\n",
      "  '\\\\hat': 88,\n",
      "  '\\\\hbar': 163,\n",
      "  '\\\\hbox': 211,\n",
      "  '\\\\hline': 390,\n",
      "  '\\\\hookrightarrow': 371,\n",
      "  '\\\\hphantom': 447,\n",
      "  '\\\\hskip': 234,\n",
      "  '\\\\hspace': 194,\n",
      "  '\\\\i': 301,\n",
      "  '\\\\imath': 246,\n",
      "  '\\\\in': 161,\n",
      "  '\\\\infty': 120,\n",
      "  '\\\\int': 75,\n",
      "  '\\\\iota': 265,\n",
      "  '\\\\it': 250,\n",
      "  '\\\\j': 382,\n",
      "  '\\\\jmath': 273,\n",
      "  '\\\\kappa': 128,\n",
      "  '\\\\ker': 379,\n",
      "  '\\\\kern': 317,\n",
      "  '\\\\l': 252,\n",
      "  '\\\\label': 237,\n",
      "  '\\\\lambda': 56,\n",
      "  '\\\\land': 362,\n",
      "  '\\\\langle': 134,\n",
      "  '\\\\large': 415,\n",
      "  '\\\\lbrace': 305,\n",
      "  '\\\\lbrack': 244,\n",
      "  '\\\\lceil': 386,\n",
      "  '\\\\ldots': 166,\n",
      "  '\\\\le': 233,\n",
      "  '\\\\leavevmode': 419,\n",
      "  '\\\\left': 34,\n",
      "  '\\\\leftarrow': 319,\n",
      "  '\\\\lefteqn': 466,\n",
      "  '\\\\leftrightarrow': 247,\n",
      "  '\\\\leq': 180,\n",
      "  '\\\\lfloor': 358,\n",
      "  '\\\\lgroup': 475,\n",
      "  '\\\\lim': 190,\n",
      "  '\\\\limits': 210,\n",
      "  '\\\\ll': 255,\n",
      "  '\\\\llap': 393,\n",
      "  '\\\\ln': 154,\n",
      "  '\\\\log': 186,\n",
      "  '\\\\longleftrightarrow': 315,\n",
      "  '\\\\longmapsto': 335,\n",
      "  '\\\\longrightarrow': 202,\n",
      "  '\\\\lower': 405,\n",
      "  '\\\\makebox': 359,\n",
      "  '\\\\mapsto': 236,\n",
      "  '\\\\mathbb': 452,\n",
      "  '\\\\mathbf': 188,\n",
      "  '\\\\mathbin': 471,\n",
      "  '\\\\mathcal': 150,\n",
      "  '\\\\mathclose': 409,\n",
      "  '\\\\mathit': 332,\n",
      "  '\\\\mathop': 342,\n",
      "  '\\\\mathopen': 416,\n",
      "  '\\\\mathrm': 189,\n",
      "  '\\\\mathsf': 329,\n",
      "  '\\\\mathstrut': 469,\n",
      "  '\\\\matrix': 239,\n",
      "  '\\\\max': 344,\n",
      "  '\\\\mbox': 139,\n",
      "  '\\\\mid': 175,\n",
      "  '\\\\min': 285,\n",
      "  '\\\\mit': 282,\n",
      "  '\\\\mkern': 376,\n",
      "  '\\\\mp': 204,\n",
      "  '\\\\mu': 21,\n",
      "  '\\\\nabla': 140,\n",
      "  '\\\\natural': 348,\n",
      "  '\\\\ne': 267,\n",
      "  '\\\\neq': 212,\n",
      "  '\\\\ni': 343,\n",
      "  '\\\\noalign': 426,\n",
      "  '\\\\nolimits': 461,\n",
      "  '\\\\nonumber': 231,\n",
      "  '\\\\normalsize': 412,\n",
      "  '\\\\not': 222,\n",
      "  '\\\\notin': 453,\n",
      "  '\\\\nu': 44,\n",
      "  '\\\\o': 258,\n",
      "  '\\\\odot': 311,\n",
      "  '\\\\of': 448,\n",
      "  '\\\\oint': 208,\n",
      "  '\\\\omega': 93,\n",
      "  '\\\\ominus': 388,\n",
      "  '\\\\oplus': 205,\n",
      "  '\\\\oslash': 410,\n",
      "  '\\\\otimes': 160,\n",
      "  '\\\\over': 41,\n",
      "  '\\\\overbrace': 346,\n",
      "  '\\\\overleftarrow': 361,\n",
      "  '\\\\overline': 142,\n",
      "  '\\\\overrightarrow': 254,\n",
      "  '\\\\overwithdelims': 437,\n",
      "  '\\\\parallel': 242,\n",
      "  '\\\\partial': 25,\n",
      "  '\\\\perp': 196,\n",
      "  '\\\\phantom': 290,\n",
      "  '\\\\phi': 42,\n",
      "  '\\\\pi': 50,\n",
      "  '\\\\pm': 110,\n",
      "  '\\\\pmatrix': 218,\n",
      "  '\\\\pmod': 404,\n",
      "  '\\\\pounds': 373,\n",
      "  '\\\\prec': 377,\n",
      "  '\\\\prime': 130,\n",
      "  '\\\\prod': 181,\n",
      "  '\\\\propto': 219,\n",
      "  '\\\\protect': 263,\n",
      "  '\\\\protectE': 457,\n",
      "  '\\\\protectZ': 442,\n",
      "  '\\\\protecte': 459,\n",
      "  '\\\\psi': 74,\n",
      "  '\\\\qquad': 132,\n",
      "  '\\\\quad': 101,\n",
      "  '\\\\raise': 336,\n",
      "  '\\\\raisebox': 378,\n",
      "  '\\\\rangle': 104,\n",
      "  '\\\\rbrace': 306,\n",
      "  '\\\\rbrack': 284,\n",
      "  '\\\\ref': 351,\n",
      "  '\\\\renewcommand': 451,\n",
      "  '\\\\rfloor': 339,\n",
      "  '\\\\rgroup': 477,\n",
      "  '\\\\rho': 78,\n",
      "  '\\\\right': 35,\n",
      "  '\\\\rightarrow': 129,\n",
      "  '\\\\rightarrowfill': 430,\n",
      "  '\\\\rightharpoonup': 368,\n",
      "  '\\\\rightleftharpoons': 395,\n",
      "  '\\\\rlap': 352,\n",
      "  '\\\\rm': 80,\n",
      "  '\\\\root': 441,\n",
      "  '\\\\rule': 465,\n",
      "  '\\\\sb': 294,\n",
      "  '\\\\scriptscriptstyle': 260,\n",
      "  '\\\\scriptsize': 272,\n",
      "  '\\\\scriptstyle': 366,\n",
      "  '\\\\sec': 385,\n",
      "  '\\\\setcounter': 460,\n",
      "  '\\\\setminus': 396,\n",
      "  '\\\\sf': 270,\n",
      "  '\\\\sharp': 314,\n",
      "  '\\\\sigma': 73,\n",
      "  '\\\\sim': 146,\n",
      "  '\\\\simeq': 197,\n",
      "  '\\\\sin': 167,\n",
      "  '\\\\sinh': 203,\n",
      "  '\\\\skew': 425,\n",
      "  '\\\\sl': 328,\n",
      "  '\\\\slash': 286,\n",
      "  '\\\\small': 293,\n",
      "  '\\\\smash': 423,\n",
      "  '\\\\sp': 248,\n",
      "  '\\\\space': 413,\n",
      "  '\\\\sqcap': 417,\n",
      "  '\\\\sqcup': 463,\n",
      "  '\\\\sqrt': 84,\n",
      "  '\\\\ss': 381,\n",
      "  '\\\\stackrel': 199,\n",
      "  '\\\\star': 183,\n",
      "  '\\\\strut': 464,\n",
      "  '\\\\subset': 229,\n",
      "  '\\\\subseteq': 326,\n",
      "  '\\\\succ': 439,\n",
      "  '\\\\succeq': 421,\n",
      "  '\\\\sum': 95,\n",
      "  '\\\\sup': 402,\n",
      "  '\\\\supset': 281,\n",
      "  '\\\\supseteq': 367,\n",
      "  '\\\\surd': 397,\n",
      "  '\\\\tag': 403,\n",
      "  '\\\\tan': 241,\n",
      "  '\\\\tanh': 235,\n",
      "  '\\\\tau': 87,\n",
      "  '\\\\text': 353,\n",
      "  '\\\\textbf': 349,\n",
      "  '\\\\textit': 375,\n",
      "  '\\\\textrm': 257,\n",
      "  '\\\\textstyle': 243,\n",
      "  '\\\\texttt': 434,\n",
      "  '\\\\textup': 363,\n",
      "  '\\\\theta': 77,\n",
      "  '\\\\thinspace': 334,\n",
      "  '\\\\tilde': 91,\n",
      "  '\\\\times': 173,\n",
      "  '\\\\tiny': 266,\n",
      "  '\\\\to': 157,\n",
      "  '\\\\triangle': 238,\n",
      "  '\\\\triangleright': 384,\n",
      "  '\\\\tt': 356,\n",
      "  '\\\\underbrace': 325,\n",
      "  '\\\\underline': 191,\n",
      "  '\\\\uparrow': 274,\n",
      "  '\\\\upsilon': 291,\n",
      "  '\\\\varepsilon': 149,\n",
      "  '\\\\varphi': 115,\n",
      "  '\\\\varpi': 280,\n",
      "  '\\\\varrho': 240,\n",
      "  '\\\\varsigma': 299,\n",
      "  '\\\\vartheta': 207,\n",
      "  '\\\\vbox': 427,\n",
      "  '\\\\vdash': 387,\n",
      "  '\\\\vec': 114,\n",
      "  '\\\\vee': 278,\n",
      "  '\\\\vert': 170,\n",
      "  '\\\\vline': 431,\n",
      "  '\\\\vphantom': 350,\n",
      "  '\\\\vrule': 456,\n",
      "  '\\\\vskip': 411,\n",
      "  '\\\\vspace': 369,\n",
      "  '\\\\wedge': 156,\n",
      "  '\\\\widehat': 187,\n",
      "  '\\\\widetilde': 165,\n",
      "  '\\\\wp': 271,\n",
      "  '\\\\xi': 102,\n",
      "  '\\\\zeta': 145,\n",
      "  '\\\\{': 123,\n",
      "  '\\\\|': 220,\n",
      "  '\\\\}': 124,\n",
      "  '\\\\~': 454,\n",
      "  ']': 47,\n",
      "  '^': 5,\n",
      "  '_': 4,\n",
      "  '_GO': 1,\n",
      "  '_PAD': 0,\n",
      "  '`': 303,\n",
      "  'a': 18,\n",
      "  'b': 51,\n",
      "  'c': 43,\n",
      "  'd': 22,\n",
      "  'e': 28,\n",
      "  'f': 57,\n",
      "  'g': 36,\n",
      "  'h': 96,\n",
      "  'i': 14,\n",
      "  'j': 37,\n",
      "  'k': 26,\n",
      "  'l': 70,\n",
      "  'm': 29,\n",
      "  'n': 23,\n",
      "  'o': 125,\n",
      "  'p': 38,\n",
      "  'q': 67,\n",
      "  'r': 24,\n",
      "  's': 53,\n",
      "  't': 32,\n",
      "  'u': 79,\n",
      "  'v': 103,\n",
      "  'w': 131,\n",
      "  'x': 19,\n",
      "  'y': 66,\n",
      "  'z': 49,\n",
      "  '{': 3,\n",
      "  '|': 46,\n",
      "  '}': 2,\n",
      "  '~': 39},\n",
      " {0: '_PAD',\n",
      "  1: '_GO',\n",
      "  2: '}',\n",
      "  3: '{',\n",
      "  4: '_',\n",
      "  5: '^',\n",
      "  6: ')',\n",
      "  7: '(',\n",
      "  8: '=',\n",
      "  9: '2',\n",
      "  10: ',',\n",
      "  11: '1',\n",
      "  12: '-',\n",
      "  13: '+',\n",
      "  14: 'i',\n",
      "  15: '.',\n",
      "  16: '0',\n",
      "  17: '\\\\frac',\n",
      "  18: 'a',\n",
      "  19: 'x',\n",
      "  20: '\\\\,',\n",
      "  21: '\\\\mu',\n",
      "  22: 'd',\n",
      "  23: 'n',\n",
      "  24: 'r',\n",
      "  25: '\\\\partial',\n",
      "  26: 'k',\n",
      "  27: 'A',\n",
      "  28: 'e',\n",
      "  29: 'm',\n",
      "  30: '\\\\alpha',\n",
      "  31: '\\\\;',\n",
      "  32: 't',\n",
      "  33: '\\\\',\n",
      "  34: '\\\\left',\n",
      "  35: '\\\\right',\n",
      "  36: 'g',\n",
      "  37: 'j',\n",
      "  38: 'p',\n",
      "  39: '~',\n",
      "  40: '3',\n",
      "  41: '\\\\over',\n",
      "  42: '\\\\phi',\n",
      "  43: 'c',\n",
      "  44: '\\\\nu',\n",
      "  45: '4',\n",
      "  46: '|',\n",
      "  47: ']',\n",
      "  48: '[',\n",
      "  49: 'z',\n",
      "  50: '\\\\pi',\n",
      "  51: 'b',\n",
      "  52: 'S',\n",
      "  53: 's',\n",
      "  54: '\\\\cal',\n",
      "  55: '\\\\delta',\n",
      "  56: '\\\\lambda',\n",
      "  57: 'f',\n",
      "  58: 'T',\n",
      "  59: 'R',\n",
      "  60: 'M',\n",
      "  61: 'N',\n",
      "  62: \"'\",\n",
      "  63: '\\\\beta',\n",
      "  64: 'L',\n",
      "  65: 'D',\n",
      "  66: 'y',\n",
      "  67: 'q',\n",
      "  68: 'B',\n",
      "  69: 'F',\n",
      "  70: 'l',\n",
      "  71: '\\\\bar',\n",
      "  72: '\\\\gamma',\n",
      "  73: '\\\\sigma',\n",
      "  74: '\\\\psi',\n",
      "  75: '\\\\int',\n",
      "  76: 'H',\n",
      "  77: '\\\\theta',\n",
      "  78: '\\\\rho',\n",
      "  79: 'u',\n",
      "  80: '\\\\rm',\n",
      "  81: 'V',\n",
      "  82: 'P',\n",
      "  83: 'C',\n",
      "  84: '\\\\sqrt',\n",
      "  85: 'E',\n",
      "  86: '/',\n",
      "  87: '\\\\tau',\n",
      "  88: '\\\\hat',\n",
      "  89: 'G',\n",
      "  90: 'X',\n",
      "  91: '\\\\tilde',\n",
      "  92: '\\\\epsilon',\n",
      "  93: '\\\\omega',\n",
      "  94: 'I',\n",
      "  95: '\\\\sum',\n",
      "  96: 'h',\n",
      "  97: 'Q',\n",
      "  98: 'J',\n",
      "  99: '\\\\Phi',\n",
      "  100: '\\\\eta',\n",
      "  101: '\\\\quad',\n",
      "  102: '\\\\xi',\n",
      "  103: 'v',\n",
      "  104: '\\\\rangle',\n",
      "  105: 'W',\n",
      "  106: '\\\\bf',\n",
      "  107: 'K',\n",
      "  108: '\\\\Lambda',\n",
      "  109: '\\\\Gamma',\n",
      "  110: '\\\\pm',\n",
      "  111: 'U',\n",
      "  112: '\\\\Delta',\n",
      "  113: 'Z',\n",
      "  114: '\\\\vec',\n",
      "  115: '\\\\varphi',\n",
      "  116: '5',\n",
      "  117: '\\\\Omega',\n",
      "  118: '*',\n",
      "  119: '\\\\equiv',\n",
      "  120: '\\\\infty',\n",
      "  121: '\\\\chi',\n",
      "  122: '\\\\dot',\n",
      "  123: '\\\\{',\n",
      "  124: '\\\\}',\n",
      "  125: 'o',\n",
      "  126: '\\\\Psi',\n",
      "  127: '6',\n",
      "  128: '\\\\kappa',\n",
      "  129: '\\\\rightarrow',\n",
      "  130: '\\\\prime',\n",
      "  131: 'w',\n",
      "  132: '\\\\qquad',\n",
      "  133: '\\\\cdot',\n",
      "  134: '\\\\langle',\n",
      "  135: '8',\n",
      "  136: ';',\n",
      "  137: '>',\n",
      "  138: '%',\n",
      "  139: '\\\\mbox',\n",
      "  140: '\\\\nabla',\n",
      "  141: '<',\n",
      "  142: '\\\\overline',\n",
      "  143: 'O',\n",
      "  144: '\\\\Sigma',\n",
      "  145: '\\\\zeta',\n",
      "  146: '\\\\sim',\n",
      "  147: ':',\n",
      "  148: '&',\n",
      "  149: '\\\\varepsilon',\n",
      "  150: '\\\\mathcal',\n",
      "  151: '\\\\dagger',\n",
      "  152: '\\\\exp',\n",
      "  153: 'Y',\n",
      "  154: '\\\\ln',\n",
      "  155: '\\\\!',\n",
      "  156: '\\\\wedge',\n",
      "  157: '\\\\to',\n",
      "  158: '\\\\:',\n",
      "  159: '\\\\ell',\n",
      "  160: '\\\\otimes',\n",
      "  161: '\\\\in',\n",
      "  162: '\\\\Pi',\n",
      "  163: '\\\\hbar',\n",
      "  164: '\\\\cdots',\n",
      "  165: '\\\\widetilde',\n",
      "  166: '\\\\ldots',\n",
      "  167: '\\\\sin',\n",
      "  168: '\\\\\\\\',\n",
      "  169: '7',\n",
      "  170: '\\\\vert',\n",
      "  171: '9',\n",
      "  172: '\\\\approx',\n",
      "  173: '\\\\times',\n",
      "  174: '$',\n",
      "  175: '\\\\mid',\n",
      "  176: '\\\\cos',\n",
      "  177: '\\\\Theta',\n",
      "  178: '\\\\end',\n",
      "  179: '\\\\begin',\n",
      "  180: '\\\\leq',\n",
      "  181: '\\\\prod',\n",
      "  182: '\\\\ast',\n",
      "  183: '\\\\star',\n",
      "  184: '!',\n",
      "  185: '\\\\cr',\n",
      "  186: '\\\\log',\n",
      "  187: '\\\\widehat',\n",
      "  188: '\\\\mathbf',\n",
      "  189: '\\\\mathrm',\n",
      "  190: '\\\\lim',\n",
      "  191: '\\\\underline',\n",
      "  192: '\\\\Big',\n",
      "  193: '\\\\dots',\n",
      "  194: '\\\\hspace',\n",
      "  195: '\\\\geq',\n",
      "  196: '\\\\perp',\n",
      "  197: '\\\\simeq',\n",
      "  198: '\\\\circ',\n",
      "  199: '\\\\stackrel',\n",
      "  200: '\\\\cosh',\n",
      "  201: '\\\\det',\n",
      "  202: '\\\\longrightarrow',\n",
      "  203: '\\\\sinh',\n",
      "  204: '\\\\mp',\n",
      "  205: '\\\\oplus',\n",
      "  206: '\\\\big',\n",
      "  207: '\\\\vartheta',\n",
      "  208: '\\\\oint',\n",
      "  209: '\\\\ddot',\n",
      "  210: '\\\\limits',\n",
      "  211: '\\\\hbox',\n",
      "  212: '\\\\neq',\n",
      "  213: '\\\\dag',\n",
      "  214: '\\\\>',\n",
      "  215: '\\\\bigg',\n",
      "  216: '\\\\ge',\n",
      "  217: '\\\\Xi',\n",
      "  218: '\\\\pmatrix',\n",
      "  219: '\\\\propto',\n",
      "  220: '\\\\|',\n",
      "  221: '\\\\bigr',\n",
      "  222: '\\\\not',\n",
      "  223: '\\\\biggl',\n",
      "  224: '\\\\Bigr',\n",
      "  225: '\\\\Bigl',\n",
      "  226: '\\\\bigl',\n",
      "  227: '\\\\biggr',\n",
      "  228: '\\\\boldmath',\n",
      "  229: '\\\\subset',\n",
      "  230: '\\\\forall',\n",
      "  231: '\\\\nonumber',\n",
      "  232: '\\\\displaystyle',\n",
      "  233: '\\\\le',\n",
      "  234: '\\\\hskip',\n",
      "  235: '\\\\tanh',\n",
      "  236: '\\\\mapsto',\n",
      "  237: '\\\\label',\n",
      "  238: '\\\\triangle',\n",
      "  239: '\\\\matrix',\n",
      "  240: '\\\\varrho',\n",
      "  241: '\\\\tan',\n",
      "  242: '\\\\parallel',\n",
      "  243: '\\\\textstyle',\n",
      "  244: '\\\\lbrack',\n",
      "  245: '\\\\Rightarrow',\n",
      "  246: '\\\\imath',\n",
      "  247: '\\\\leftrightarrow',\n",
      "  248: '\\\\sp',\n",
      "  249: '\\\\gg',\n",
      "  250: '\\\\it',\n",
      "  251: '\\\\cong',\n",
      "  252: '\\\\l',\n",
      "  253: '\\\\d',\n",
      "  254: '\\\\overrightarrow',\n",
      "  255: '\\\\ll',\n",
      "  256: '\\\\check',\n",
      "  257: '\\\\textrm',\n",
      "  258: '\\\\o',\n",
      "  259: '\\\\Upsilon',\n",
      "  260: '\\\\scriptscriptstyle',\n",
      "  261: '\\\\bot',\n",
      "  262: '\\\\bigoplus',\n",
      "  263: '\\\\protect',\n",
      "  264: '\\\\L',\n",
      "  265: '\\\\iota',\n",
      "  266: '\\\\tiny',\n",
      "  267: '\\\\ne',\n",
      "  268: '\\\\#',\n",
      "  269: '\\\\cup',\n",
      "  270: '\\\\sf',\n",
      "  271: '\\\\wp',\n",
      "  272: '\\\\scriptsize',\n",
      "  273: '\\\\jmath',\n",
      "  274: '\\\\uparrow',\n",
      "  275: '\\\\choose',\n",
      "  276: '\\\\bullet',\n",
      "  277: '\\\\cap',\n",
      "  278: '\\\\vee',\n",
      "  279: '\\\\cot',\n",
      "  280: '\\\\varpi',\n",
      "  281: '\\\\supset',\n",
      "  282: '\\\\mit',\n",
      "  283: '\\\\Im',\n",
      "  284: '\\\\rbrack',\n",
      "  285: '\\\\min',\n",
      "  286: '\\\\slash',\n",
      "  287: '\\\\O',\n",
      "  288: '\\\\downarrow',\n",
      "  289: '\\\\coth',\n",
      "  290: '\\\\phantom',\n",
      "  291: '\\\\upsilon',\n",
      "  292: '\\\\Re',\n",
      "  293: '\\\\small',\n",
      "  294: '\\\\sb',\n",
      "  295: '\\\\S',\n",
      "  296: '\\\\Longleftrightarrow',\n",
      "  297: '\\\\Leftrightarrow',\n",
      "  298: '\\\\breve',\n",
      "  299: '\\\\varsigma',\n",
      "  300: '\\\\arg',\n",
      "  301: '\\\\i',\n",
      "  302: '\\\\arctan',\n",
      "  303: '`',\n",
      "  304: '\\\\doteq',\n",
      "  305: '\\\\lbrace',\n",
      "  306: '\\\\rbrace',\n",
      "  307: '\\\\bigotimes',\n",
      "  308: '\\\\dim',\n",
      "  309: '\\\\bigcup',\n",
      "  310: '\\\\bigtriangleup',\n",
      "  311: '\\\\odot',\n",
      "  312: '\\\\Vert',\n",
      "  313: '\\\\Biggr',\n",
      "  314: '\\\\sharp',\n",
      "  315: '\\\\longleftrightarrow',\n",
      "  316: '\\\\Biggl',\n",
      "  317: '\\\\kern',\n",
      "  318: '\\\\deg',\n",
      "  319: '\\\\leftarrow',\n",
      "  320: '\"',\n",
      "  321: '\\\\P',\n",
      "  322: '\\\\/',\n",
      "  323: '\\\\Bigg',\n",
      "  324: '\\\\buildrel',\n",
      "  325: '\\\\underbrace',\n",
      "  326: '\\\\subseteq',\n",
      "  327: '\\\\atop',\n",
      "  328: '\\\\sl',\n",
      "  329: '\\\\mathsf',\n",
      "  330: '\\\\Longrightarrow',\n",
      "  331: '\\\\eqno',\n",
      "  332: '\\\\mathit',\n",
      "  333: '\\\\cases',\n",
      "  334: '\\\\thinspace',\n",
      "  335: '\\\\longmapsto',\n",
      "  336: '\\\\raise',\n",
      "  337: '\\\\backslash',\n",
      "  338: '\\\\cdotp',\n",
      "  339: '\\\\rfloor',\n",
      "  340: '\\\\enspace',\n",
      "  341: '\\\\emptyset',\n",
      "  342: '\\\\mathop',\n",
      "  343: '\\\\ni',\n",
      "  344: '\\\\max',\n",
      "  345: '\\\\colon',\n",
      "  346: '\\\\overbrace',\n",
      "  347: '\\\\acute',\n",
      "  348: '\\\\natural',\n",
      "  349: '\\\\textbf',\n",
      "  350: '\\\\vphantom',\n",
      "  351: '\\\\ref',\n",
      "  352: '\\\\rlap',\n",
      "  353: '\\\\text',\n",
      "  354: '\\\\Large',\n",
      "  355: '\\\\enskip',\n",
      "  356: '\\\\tt',\n",
      "  357: '\\\\bmod',\n",
      "  358: '\\\\lfloor',\n",
      "  359: '\\\\makebox',\n",
      "  360: '?',\n",
      "  361: '\\\\overleftarrow',\n",
      "  362: '\\\\land',\n",
      "  363: '\\\\textup',\n",
      "  364: '\\\\aleph',\n",
      "  365: '\\\\footnotesize',\n",
      "  366: '\\\\scriptstyle',\n",
      "  367: '\\\\supseteq',\n",
      "  368: '\\\\rightharpoonup',\n",
      "  369: '\\\\vspace',\n",
      "  370: '\\\\diamond',\n",
      "  371: '\\\\hookrightarrow',\n",
      "  372: '\\\\amalg',\n",
      "  373: '\\\\pounds',\n",
      "  374: '\\\\_',\n",
      "  375: '\\\\textit',\n",
      "  376: '\\\\mkern',\n",
      "  377: '\\\\prec',\n",
      "  378: '\\\\raisebox',\n",
      "  379: '\\\\ker',\n",
      "  380: '\\\\bigcap',\n",
      "  381: '\\\\ss',\n",
      "  382: '\\\\j',\n",
      "  383: '\\\\diamondsuit',\n",
      "  384: '\\\\triangleright',\n",
      "  385: '\\\\sec',\n",
      "  386: '\\\\lceil',\n",
      "  387: '\\\\vdash',\n",
      "  388: '\\\\ominus',\n",
      "  389: '\\\\arcsin',\n",
      "  390: '\\\\hline',\n",
      "  391: '\\\\arccos',\n",
      "  392: '\\\\bigtriangledown',\n",
      "  393: '\\\\llap',\n",
      "  394: '\\\\bigm',\n",
      "  395: '\\\\rightleftharpoons',\n",
      "  396: '\\\\setminus',\n",
      "  397: '\\\\surd',\n",
      "  398: '\\\\flat',\n",
      "  399: '\\\\$',\n",
      "  400: '\\\\-',\n",
      "  401: '\\\\b',\n",
      "  402: '\\\\sup',\n",
      "  403: '\\\\tag',\n",
      "  404: '\\\\pmod',\n",
      "  405: '\\\\lower',\n",
      "  406: '\\\\SS',\n",
      "  407: '\\\\ddag',\n",
      "  408: '\\\\c',\n",
      "  409: '\\\\mathclose',\n",
      "  410: '\\\\oslash',\n",
      "  411: '\\\\vskip',\n",
      "  412: '\\\\normalsize',\n",
      "  413: '\\\\space',\n",
      "  414: '\\\\do',\n",
      "  415: '\\\\large',\n",
      "  416: '\\\\mathopen',\n",
      "  417: '\\\\sqcap',\n",
      "  418: '\\\\exists',\n",
      "  419: '\\\\leavevmode',\n",
      "  420: '\\\\grave',\n",
      "  421: '\\\\succeq',\n",
      "  422: '\\\\bigsqcup',\n",
      "  423: '\\\\smash',\n",
      "  424: '\\\\ensuremath',\n",
      "  425: '\\\\skew',\n",
      "  426: '\\\\noalign',\n",
      "  427: '\\\\vbox',\n",
      "  428: '\\\\Longleftarrow',\n",
      "  429: '\\\\ft',\n",
      "  430: '\\\\rightarrowfill',\n",
      "  431: '\\\\vline',\n",
      "  432: '\\\\arrowvert',\n",
      "  433: '\\\\dicov',\n",
      "  434: '\\\\texttt',\n",
      "  435: '\\\\bigwedge',\n",
      "  436: '\\\\biggm',\n",
      "  437: '\\\\overwithdelims',\n",
      "  438: '\\\\%',\n",
      "  439: '\\\\succ',\n",
      "  440: '@',\n",
      "  441: '\\\\root',\n",
      "  442: '\\\\protectZ',\n",
      "  443: '\\\\atopwithdelims',\n",
      "  444: '\\\\AA',\n",
      "  445: '\\\\footnotemark',\n",
      "  446: '\\\\&',\n",
      "  447: '\\\\hphantom',\n",
      "  448: '\\\\of',\n",
      "  449: '\\\\def',\n",
      "  450: '\\\\^',\n",
      "  451: '\\\\renewcommand',\n",
      "  452: '\\\\mathbb',\n",
      "  453: '\\\\notin',\n",
      "  454: '\\\\~',\n",
      "  455: '\\\\ddagger',\n",
      "  456: '\\\\vrule',\n",
      "  457: '\\\\protectE',\n",
      "  458: '\\\\LARGE',\n",
      "  459: '\\\\protecte',\n",
      "  460: '\\\\setcounter',\n",
      "  461: '\\\\nolimits',\n",
      "  462: '\\\\brack',\n",
      "  463: '\\\\sqcup',\n",
      "  464: '\\\\strut',\n",
      "  465: '\\\\rule',\n",
      "  466: '\\\\lefteqn',\n",
      "  467: '\\\\ditil',\n",
      "  468: '\\\\case',\n",
      "  469: '\\\\mathstrut',\n",
      "  470: '\\\\Box',\n",
      "  471: '\\\\mathbin',\n",
      "  472: '\\\\fbox',\n",
      "  473: '\\\\Huge',\n",
      "  474: '\\\\csc',\n",
      "  475: '\\\\lgroup',\n",
      "  476: '\\\\bigvee',\n",
      "  477: '\\\\rgroup'},\n",
      " 476)\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(sentences, max_vocab_size=None, verbose = False):\n",
    "    word_counter = Counter()\n",
    "    vocab = dict()\n",
    "    reverse_vocab = dict()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            tokens = tokenizer_no_label(sentence)\n",
    "            word_counter.update(tokens)\n",
    "        except:\n",
    "            if verbose:\n",
    "                print(sentence)\n",
    "            continue\n",
    "        \n",
    "    if max_vocab_size is None:\n",
    "        max_vocab_size = len(word_counter)\n",
    "\n",
    "    vocab['_GO'] = 1\n",
    "    vocab['_PAD'] = 0\n",
    "    vocab_idx = 2\n",
    "    for key, value in word_counter.most_common(max_vocab_size):\n",
    "        vocab[key] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "        \n",
    "    for key, value in vocab.items():\n",
    "        reverse_vocab[value] = key\n",
    "            \n",
    "    return vocab, reverse_vocab, max_vocab_size\n",
    "\n",
    "\n",
    "pprint(build_vocab(all_target_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target vocabulary size: 476\n"
     ]
    }
   ],
   "source": [
    "# enc_vocab, enc_reverse_vocab, enc_vocab_size = build_vocab(all_input_sentences)\n",
    "dec_vocab, dec_reverse_vocab, dec_vocab_size = build_vocab(all_target_sentences)\n",
    "\n",
    "# print('input vocabulary size:', enc_vocab_size)\n",
    "print('target vocabulary size:', dec_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V 81\n",
      "= 8\n",
      "\\frac 17\n",
      "{ 3\n",
      "1 11\n",
      "} 2\n",
      "{ 3\n",
      "3 40\n",
      "} 2\n",
      "\\pi 50\n",
      "r 24\n",
      "^ 5\n",
      "2 9\n",
      "h 96\n"
     ]
    }
   ],
   "source": [
    "def token2idx(word, vocab):\n",
    "    return vocab[word]\n",
    "\n",
    "for token in tokenizer_no_label('V = \\\\frac{1}{3} \\\\label{1} \\\\pi r^2 h'):\n",
    "    print(token, token2idx(token, dec_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_sentence_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V = \\frac{1}{3}\\label{label}\\pi r^2 h\n",
      "([1, 81, 8, 17, 3, 11, 2, 3, 40, 2, 50, 24, 5, 9, 96, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 14)\n"
     ]
    }
   ],
   "source": [
    "def sent2idx(sent, vocab, max_sentence_length, verbose=False):\n",
    "    try:\n",
    "        tokens = tokenizer_no_label(sent)[:max_sentence_length]\n",
    "    except:\n",
    "        if verbose:\n",
    "            print(sent)\n",
    "        tokens = ['_PAD']*max_sentence_length\n",
    "    current_length = len(tokens)\n",
    "    pad_length = max_sentence_length - current_length\n",
    "    return [1] + [token2idx(token, vocab) for token in tokens] + [0] * pad_length, current_length\n",
    "#     else:\n",
    "#         return [token2idx(token, vocab) for token in tokens] + [0] * pad_length, current_length\n",
    "\n",
    "# Enc Example\n",
    "# print('Hi What is your name?')\n",
    "# print(sent2idx('Hi What is your name?'))\n",
    "\n",
    "# Dec Example\n",
    "print('V = \\\\frac{1}{3}\\\\label{label}\\\\pi r^2 h')\n",
    "print(sent2idx('V = \\\\frac{1}{3}\\\\label{label}\\\\pi r^2 h', vocab=dec_vocab, max_sentence_length=dec_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def idx2token(idx, reverse_vocab):\n",
    "    return reverse_vocab[idx]\n",
    "\n",
    "def idx2sent(indices, reverse_vocab=dec_reverse_vocab, replace_pad = True):\n",
    "    sent = \" \".join([idx2token(idx, reverse_vocab) for idx in indices])\n",
    "    if replace_pad:\n",
    "        return sent.replace(' _PAD','')\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters / Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 37)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h,w,_ = input_batches[0][0].shape\n",
    "h,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dec_reverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DemoConfig:\n",
    "    \n",
    "    # Model\n",
    "    hidden_size = 256\n",
    "    dec_emb_size = len(dec_reverse_vocab)\n",
    "    dec_sent_length = 50\n",
    "    enc_emb_size = 256\n",
    "    attn_size = 256\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell\n",
    "    beam_width = 5\n",
    "    num_filters = [32, 64, 128]\n",
    "    img_height = h\n",
    "    img_width = w\n",
    "    L = 124\n",
    "    \n",
    "    # Training\n",
    "    optimizer = tf.train.RMSPropOptimizer\n",
    "    n_epoch = 10\n",
    "    learning_rate = 0.001\n",
    "    conv_dropout = 0.25\n",
    "    dense_dropout = 0.5\n",
    "    \n",
    "    # Tokens\n",
    "    start_token = 1 # GO\n",
    "    end_token = 0 # PAD\n",
    "\n",
    "    # Checkpoint Path\n",
    "    ckpt_dir = './ckpt_dir/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.embedding_ops.embedding_lookup>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, config, mode='training'):\n",
    "        assert mode in ['training', 'evaluation', 'inference']\n",
    "        self.mode = mode\n",
    "\n",
    "        # Model\n",
    "#         self.hidden_size = config.hidden_size\n",
    "        self.dec_emb_size = config.dec_emb_size\n",
    "        self.enc_emb_size = config.enc_emb_size\n",
    "        self.dec_sent_length = config.dec_sent_length\n",
    "        self.attn_size = config.attn_size\n",
    "        self.cell = config.cell\n",
    "        self.beam_width = config.beam_width\n",
    "        self.num_filters = config.num_filters\n",
    "        self.img_height = config.img_height\n",
    "        self.img_width = config.img_width\n",
    "        \n",
    "        # Training\n",
    "        self.optimizer = config.optimizer\n",
    "        self.n_epoch = config.n_epoch\n",
    "        self.learning_rate = config.learning_rate\n",
    "        \n",
    "        \n",
    "        # Tokens\n",
    "        self.start_token = config.start_token\n",
    "        self.end_token = config.end_token\n",
    "        \n",
    "        # Checkpoint Path\n",
    "        self.ckpt_dir = config.ckpt_dir\n",
    "        \n",
    "    def add_placeholders(self):\n",
    "\n",
    "        if self.mode == 'training':\n",
    "            self.dec_inputs = tf.placeholder(\n",
    "                tf.int32,\n",
    "                shape=[None, self.dec_sent_length+1],\n",
    "                name='target_sentences')\n",
    "\n",
    "            self.dec_sequence_length = tf.placeholder(\n",
    "                tf.int32,\n",
    "                shape=[None,],\n",
    "                name='target_sequence_length')\n",
    "        \n",
    "        self.vgg_features = tf.placeholder(tf.float32,\n",
    "                                         shape = [None, self.img_height, self.img_width, self.enc_emb_size])\n",
    "        \n",
    "        self.imbeds = tf.reshape(self.vgg_features,\n",
    "                                      shape = [-1,self.img_height*self.img_width, self.enc_emb_size])\n",
    "        \n",
    "            \n",
    "            \n",
    "    def add_decoder(self):\n",
    "        with tf.variable_scope('Decoder') as scope:\n",
    "            # get dynamic batch_size\n",
    "            batch_size = tf.shape(self.imbeds)[0]\n",
    "\n",
    "            dec_cell = self.cell(self.enc_emb_size)\n",
    "            \n",
    "            attn_mech = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                num_units=self.attn_size,\n",
    "                memory=self.imbeds,\n",
    "                name='BahdanauAttention')\n",
    "\n",
    "            dec_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell=dec_cell,\n",
    "                attention_mechanism=attn_mech,\n",
    "                name='Attention_Wrapper')\n",
    "            \n",
    "\n",
    "            # output projection (replacing `OutputProjectionWrapper`)\n",
    "            output_layer = Dense(dec_vocab_size+2, name='output_projection')\n",
    "            \n",
    "            if self.mode == 'training':\n",
    "\n",
    "                # maxium unrollings in current batch = max(dec_sent_len) + 1(GO symbol)\n",
    "                max_dec_len = tf.reduce_max(self.dec_sequence_length+1, name='max_dec_len')\n",
    "                \n",
    "                embedding_matrix = np.float32(np.eye(self.dec_emb_size))\n",
    "                \n",
    "                \n",
    "#                 dec_emb_inputs = tf.one_hot(self.dec_inputs, depth = self.dec_emb_size)\n",
    "\n",
    "                dec_emb_inputs = tf.nn.embedding_lookup(params=embedding_matrix, ids=self.dec_inputs)\n",
    "    \n",
    "                training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                    inputs=dec_emb_inputs,\n",
    "                    sequence_length=self.dec_sequence_length+1,\n",
    "                    time_major=False,\n",
    "                    name='training_helper')\n",
    "\n",
    "                training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell=dec_cell,\n",
    "                    helper=training_helper,\n",
    "                    initial_state = dec_cell.zero_state(batch_size, tf.float32),\n",
    "                    output_layer=output_layer) \n",
    "\n",
    "                train_dec_outputs, train_dec_last_state, train_dec_last_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    training_decoder,\n",
    "                    output_time_major=False,\n",
    "                    impute_finished=True,\n",
    "                    maximum_iterations=max_dec_len)\n",
    "                \n",
    "                # dec_outputs: collections.namedtuple(rnn_outputs, sample_id)\n",
    "                # dec_outputs.rnn_output: [batch_size x max(dec_sequence_len) x dec_vocab_size+2], tf.float32\n",
    "                # dec_outputs.sample_id [batch_size], tf.int32\n",
    "                \n",
    "                # logits: [batch_size x max_dec_len x dec_vocab_size+2]\n",
    "                logits = tf.identity(train_dec_outputs.rnn_output, name='logits')\n",
    "                \n",
    "                # targets: [batch_size x max_dec_len x dec_vocab_size+2]\n",
    "                targets = tf.slice(self.dec_inputs, [0, 0], [-1, max_dec_len], 'targets')\n",
    "                \n",
    "                # masks: [batch_size x max_dec_len]\n",
    "                # => ignore outputs after `dec_senquence_length+1` when calculating loss\n",
    "                masks = tf.sequence_mask(self.dec_sequence_length+1, \n",
    "                                         max_dec_len, dtype=tf.float32, name='masks')\n",
    "                \n",
    "                # Control loss dimensions with `average_across_timesteps` and `average_across_batch`\n",
    "                # internal: `tf.nn.sparse_softmax_cross_entropy_with_logits`\n",
    "                self.batch_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                    logits=logits,\n",
    "                    targets=targets,\n",
    "                    weights=masks,\n",
    "                    name='batch_loss')\n",
    "                \n",
    "                # prediction sample for validation\n",
    "                self.valid_predictions = tf.identity(train_dec_outputs.sample_id, name='valid_preds')\n",
    "\n",
    "                # List of training variables\n",
    "                # self.training_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "            \n",
    "            elif self.mode == 'inference':\n",
    "            \n",
    "                start_tokens = tf.tile(tf.constant([self.start_token], dtype=tf.int32), \n",
    "                                       [batch_size], name='start_tokens')\n",
    "            \n",
    "                inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper( \n",
    "#                     embedding=self.dec_Wemb,\n",
    "                    embedding = tf.one_hot(np.arange(self.dec_emb_size),depth = self.dec_emb_size),\n",
    "                    start_tokens=start_tokens,\n",
    "                    end_token=self.end_token)\n",
    "                \n",
    "                inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell=dec_cell,\n",
    "                    helper=inference_helper,\n",
    "                    initial_state=dec_cell.zero_state(batch_size, tf.float32),\n",
    "                    output_layer=output_layer)\n",
    "                \n",
    "                infer_dec_outputs, infer_dec_last_state, infer_dec_last_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    inference_decoder,\n",
    "                    output_time_major=False,\n",
    "                    impute_finished=True,\n",
    "                    maximum_iterations=self.dec_sent_length)\n",
    "                \n",
    "                # [batch_size x dec_sentence_length], tf.int32\n",
    "                self.predictions = tf.identity(infer_dec_outputs.sample_id, name='predictions')\n",
    "                # equivalent to tf.argmax(infer_dec_outputs.rnn_output, axis=2, name='predictions')\n",
    "                \n",
    "                # List of training variables\n",
    "                # self.training_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "\n",
    "    def add_training_op(self):\n",
    "        self.training_op = self.optimizer(self.learning_rate, name='training_op').minimize(self.batch_loss)\n",
    "        \n",
    "    def save(self, sess, var_list=None, save_path=None):\n",
    "        print('Saving model at {save_path}'.format(save_path=save_path))\n",
    "        if hasattr(self, 'training_variables'):\n",
    "            var_list = self.training_variables\n",
    "        saver = tf.train.Saver(var_list)\n",
    "        saver.save(sess, save_path, write_meta_graph=False)\n",
    "        \n",
    "    def restore(self, sess, var_list=None, ckpt_path=None):\n",
    "        if hasattr(self, 'training_variables'):\n",
    "            var_list = self.training_variables\n",
    "        self.restorer = tf.train.Saver(var_list)\n",
    "        self.restorer.restore(sess, ckpt_path)\n",
    "        print('Restore Finished!')\n",
    "        \n",
    "    def summary(self):\n",
    "        summary_writer = tf.summary.FileWriter(\n",
    "            logdir=self.ckpt_dir,\n",
    "            graph=tf.get_default_graph())\n",
    "        \n",
    "    def build(self):\n",
    "        self.add_placeholders()\n",
    "        self.add_decoder()\n",
    "\n",
    "    def train(self, sess, data, from_scratch=False,\n",
    "              load_ckpt=None, save_path=None):\n",
    "        \n",
    "        # Restore Checkpoint\n",
    "        if from_scratch is False and os.path.isfile(load_ckpt):\n",
    "            self.restore(sess, load_ckpt)\n",
    "    \n",
    "        # Add Optimizer to current graph\n",
    "        self.add_training_op()\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        input_batches, target_batches = data\n",
    "        loss_history = []\n",
    "        t = timeit.default_timer()\n",
    "        for epoch in range(self.n_epoch):\n",
    "\n",
    "            all_preds = []\n",
    "            epoch_loss = 0\n",
    "            batch_num = 0\n",
    "        \n",
    "            for input_batch, target_batch in zip(input_batches, target_batches):\n",
    "                batch_num+=1\n",
    "                target_batch_tokens = []\n",
    "                dec_sentence_lengths = []\n",
    "\n",
    "                for target_sent in target_batch:\n",
    "                    tokens, sent_len = sent2idx(target_sent,\n",
    "                                 vocab=dec_vocab,\n",
    "                                 max_sentence_length=self.dec_sent_length)\n",
    "                    target_batch_tokens.append(tokens)\n",
    "                    dec_sentence_lengths.append(sent_len)\n",
    "       \n",
    "                # Evaluate 3 ops in the graph\n",
    "                # => valid_predictions, loss, training_op(optimzier)\n",
    "            \n",
    "                batch_preds, batch_loss, _ = sess.run(\n",
    "                    [self.valid_predictions, self.batch_loss, self.training_op],\n",
    "                    feed_dict={\n",
    "                        self.vgg_features: input_batch,\n",
    "                        self.dec_inputs: target_batch_tokens,\n",
    "                        self.dec_sequence_length: dec_sentence_lengths,\n",
    "                    })\n",
    "                # loss_history.append(batch_loss)\n",
    "                epoch_loss += batch_loss\n",
    "                all_preds.append(batch_preds)\n",
    "                t_ = timeit.default_timer()\n",
    "                if not (batch_num % 25):\n",
    "                    print(epoch, batch_num)\n",
    "                    print(t_ - t)\n",
    "                t = t_\n",
    "                \n",
    "            loss_history.append(epoch_loss)\n",
    "\n",
    "            # Logging every 400 epochs\n",
    "            if epoch % 1 == 0:\n",
    "                print('Epoch', epoch)\n",
    "                for input_batch, target_batch, batch_preds in zip(input_batches[0:1][:5], target_batches[0:1][:5], all_preds[0:1][:5]):\n",
    "                    for input_img, target_sent, pred in zip(input_batch, target_batch, batch_preds):\n",
    "                        \n",
    "                        print('\\tPrediction:', idx2sent(pred, reverse_vocab=dec_reverse_vocab).replace(' _PAD',''))\n",
    "                        print('\\tTarget:, {target_sent}'.format(target_sent=replace_label(target_sent)))\n",
    "                print('\\tepoch loss: {epoch_loss:.2f}\\n'.format(epoch_loss=epoch_loss))\n",
    "                \n",
    "            if save_path:\n",
    "                print('Saving')\n",
    "                self.save(sess, save_path=save_path+'epoch_%i_attention'%(epoch))\n",
    "\n",
    "        return loss_history\n",
    "    \n",
    "    def inference(self, sess, data, load_ckpt):\n",
    "        \n",
    "        self.restore(sess, ckpt_path=load_ckpt)\n",
    "        \n",
    "        input_batch, target_batch = data\n",
    "\n",
    "        batch_preds = []\n",
    "        batch_tokens = []\n",
    "        batch_sent_lens = []\n",
    "        \n",
    "            \n",
    "        batch_preds = sess.run(\n",
    "            self.predictions,\n",
    "            feed_dict={\n",
    "                self.vgg_features: input_batch\n",
    "            })\n",
    "        for input_img, target_sent, pred in zip(input_batch, target_batch, batch_preds):\n",
    "            plt.imshow(input_img[:,:,0],cmap='gray')\n",
    "            print('Prediction:', idx2sent(pred, reverse_vocab=dec_reverse_vocab))\n",
    "            print('Target:', target_sent, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if models are correctly built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model built!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = DemoConfig()\n",
    "model = Seq2SeqModel(config, mode='training')\n",
    "model.build()\n",
    "# model.summary()\n",
    "print('Training model built!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference model built!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = DemoConfig()\n",
    "model = Seq2SeqModel(config, mode='inference')\n",
    "model.build()\n",
    "# model.summary()\n",
    "print('Inference model built!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_inp = np.array(input_batches[0][:10]).reshape((5,2,6,37,256))\n",
    "small_tar = np.array(target_batches[0][:10]).reshape((5,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()     \n",
    "with tf.Session() as sess:\n",
    "    config = DemoConfig()\n",
    "    model = Seq2SeqModel(config, mode='training')\n",
    "    model.build()\n",
    "    data = (input_batches, target_batches)\n",
    "#     data = (small_inp, small_tar)\n",
    "    loss_history = model.train(sess, data, from_scratch=True, \n",
    "        load_ckpt=model.ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(range(model.n_epoch), loss_history)\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Global step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = Seq2SeqModel(config, mode='inference')\n",
    "    model.build()\n",
    "    for input_batch, target_batch in zip(input_batches, target_batches):\n",
    "        data = (input_batch, target_batch)\n",
    "        model.inference(sess, data, load_ckpt=model.ckpt_dir+'epoch_{model.n_epoch}_attention'.format(model=model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust inference without scheduled sampling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
