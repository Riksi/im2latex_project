{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To plot learning curve graph\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for pretty print\n",
    "from pprint import pprint\n",
    "\n",
    "# for tokenizer\n",
    "import re\n",
    "\n",
    "# for word counter in vocabulary dictionary\n",
    "from collections import Counter\n",
    "\n",
    "# for checkpoint paths\n",
    "import os\n",
    "\n",
    "# for fancy progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# for output_projection\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# for initial attention (not required ver1.2+)\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'eq_images/cropped'\n",
    "files = list(set(os.listdir(os.path.join(path,'images'))) - set(['.DS_Store']))\n",
    "imbeds = list(set(os.listdir('imbeddings2')) - set(['.DS_Store']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(path, '/Users/user/eecs/Deep_Learning/im2latex/im2latex_train.lst')) as fl:\n",
    "    ids = fl.readlines()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_name_pairs = [(id_[:-1].split())[:-1] for id_ in ids]\n",
    "imbed2id = dict([(name+'.npy',int(id_)) for id_, name in id_name_pairs])\n",
    "im2id = dict([(name+'.png',int(id_)) for id_, name in id_name_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/Users/user/eecs/Deep_Learning/im2latex/im2latex_formulas.lst','rb') as f:\n",
    "    eqs = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eqs = np.array(eqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_eqs = eqs[list(map(imbed2id.get,imbeds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40006"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice = np.random.randint(0,len(files))\n",
    "choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1442baeb8>"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAB9CAYAAAA/bGDPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZ0QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x132968860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(plt.imread(os.path.join(path,'images',imbeds[choice][:-4]+'.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\\\Gamma _{R\\\\mu }^{abc}(p,q)\\\\mid _{p=q,p^2=\\\\mu ^2}=gf^{abc}p_\\\\mu\\n'"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eqs[choice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'y']"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    tokenizer_ = RegexpTokenizer('\\\\\\\\[A-Za-z]+|\\\\\\\\\\S|\\[A-Za-z]+|\\d|\\S')\n",
    "    if type(sentence) is not str:\n",
    "        sentence = sentence.decode()\n",
    "    return tokenizer_.tokenize(sentence)\n",
    "\n",
    "# Example\n",
    "tokenizer(\"xy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'y', '+', 'z']"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "def replace_label(sentence):\n",
    "    if type(sentence) is not str:\n",
    "        sentence = sentence.decode()\n",
    "    return re.sub(string=sentence,\n",
    "                      pattern=r'\\\\label\\{[^\\}]+\\}',\n",
    "                      repl='')\n",
    "\n",
    "\n",
    "def tokenizer_no_label(sentence):\n",
    "    tokenizer_ = RegexpTokenizer('\\\\\\\\[A-Za-z]+|\\\\\\\\\\S|\\[A-Za-z]+|\\d|\\S')\n",
    "    sentence = replace_label(sentence)\n",
    "    return tokenizer_.tokenize(sentence)\n",
    "\n",
    "# Example\n",
    "tokenizer_no_label(\"xy \\\\label{this is a label} + z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_length(eq):\n",
    "    try:\n",
    "        return len(tokenizer_no_label(eq))\n",
    "    except:\n",
    "        return np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be lazy and retain a multiple of 128 for quick batching without iterators and generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32128"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ex= 128*(len(imgs)//128)\n",
    "num_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    try:\n",
    "        with open('data.pkl','rb') as f:\n",
    "            print('Loading...')\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print('Creating...')\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        data = train_test_split(imbeds, train_eqs, test_size=0.2)\n",
    "        with open('data.pkl' ,'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n"
     ]
    }
   ],
   "source": [
    "x_tr, x_val, y_tr, y_val = get_data()\n",
    "\n",
    "imgs = [np.load(os.path.join('imbeddings2',imbed)) for imbed in x_tr]\n",
    "batch_size = 128\n",
    "\n",
    "target_batches = [y_tr[i:i+batch_size] for i in range(0,len(y_tr),batch_size)]\n",
    "input_batches = [imgs[i:i+batch_size] for i in range(0,len(x_tr),batch_size)]\n",
    "\n",
    "all_target_sentences = []\n",
    "for target_batch in target_batches:\n",
    "    all_target_sentences.extend(target_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(sentences, max_vocab_size=None, verbose = False):\n",
    "    word_counter = Counter()\n",
    "    vocab = defaultdict(int)\n",
    "    reverse_vocab = dict()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            tokens = tokenizer_no_label(sentence)\n",
    "            word_counter.update(tokens)\n",
    "        except:\n",
    "            if verbose:\n",
    "                print(sentence)\n",
    "            continue\n",
    "        \n",
    "    if max_vocab_size is None:\n",
    "        max_vocab_size = len(word_counter)\n",
    "\n",
    "    vocab['_GO'] = 2\n",
    "    vocab['_PAD'] = 1\n",
    "    vocab_idx = 3\n",
    "    for key, value in word_counter.most_common(max_vocab_size):\n",
    "        vocab[key] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "        \n",
    "    for key, value in vocab.items():\n",
    "        reverse_vocab[value] = key\n",
    "    \n",
    "    reverse_vocab[0] = '_UNK'\n",
    "    \n",
    "    return vocab, reverse_vocab, max_vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target vocabulary size: 481\n"
     ]
    }
   ],
   "source": [
    "# enc_vocab, enc_reverse_vocab, enc_vocab_size = build_vocab(all_input_sentences)\n",
    "dec_vocab, dec_reverse_vocab, dec_vocab_size = build_vocab(all_target_sentences)\n",
    "\n",
    "# print('input vocabulary size:', enc_vocab_size)\n",
    "print('target vocabulary size:', dec_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token2idx(word, vocab):\n",
    "    return vocab[word]\n",
    "\n",
    "# for token in tokenizer_no_label('V = \\\\frac{1}{3} \\\\label{1} \\\\pi r^2 h'):\n",
    "#     print(token, token2idx(token, dec_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_sentence_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V = \\frac{1}{3}\\label{label}\\pi r^2 h\n",
      "([1, 81, 9, 18, 4, 12, 3, 4, 41, 3, 50, 25, 6, 10, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 14)\n"
     ]
    }
   ],
   "source": [
    "def sent2idx(sent, vocab, max_sentence_length, verbose=False):\n",
    "    try:\n",
    "        tokens = tokenizer_no_label(sent)[:max_sentence_length]\n",
    "    except:\n",
    "        if verbose:\n",
    "            print(sent)\n",
    "        tokens = ['_PAD']*max_sentence_length\n",
    "    current_length = len(tokens)\n",
    "    pad_length = max_sentence_length - current_length\n",
    "    return [1] + [token2idx(token, vocab) for token in tokens] + [0] * pad_length, current_length\n",
    "#     else:\n",
    "#         return [token2idx(token, vocab) for token in tokens] + [0] * pad_length, current_length\n",
    "\n",
    "# Enc Example\n",
    "# print('Hi What is your name?')\n",
    "# print(sent2idx('Hi What is your name?'))\n",
    "\n",
    "# Dec Example\n",
    "print('V = \\\\frac{1}{3}\\\\label{label}\\\\pi r^2 h')\n",
    "print(sent2idx('V = \\\\frac{1}{3}\\\\label{label}\\\\pi r^2 h', vocab=dec_vocab, max_sentence_length=dec_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def idx2token(idx, reverse_vocab):\n",
    "    return reverse_vocab[idx]\n",
    "\n",
    "def idx2sent(indices, reverse_vocab=dec_reverse_vocab, replace_pad = True):\n",
    "    sent = \" \".join([idx2token(idx, reverse_vocab) for idx in indices])\n",
    "    if replace_pad:\n",
    "        return sent.replace(' _PAD','')\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters / Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 37)"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h,w,_ = input_batches[0][0].shape\n",
    "h,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dec_reverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DemoConfig:\n",
    "    \n",
    "    # Model\n",
    "    hidden_size = 256\n",
    "    dec_emb_size = 500 #len(dec_reverse_vocab)\n",
    "    dec_sent_length = 50\n",
    "    enc_emb_size = 256\n",
    "    attn_size = 64 #256\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell\n",
    "    beam_width = 5\n",
    "    num_filters = [32, 64, 128]\n",
    "    img_height = h\n",
    "    img_width = w\n",
    "    L = 124\n",
    "    \n",
    "    # Training\n",
    "    optimizer = tf.train.RMSPropOptimizer\n",
    "    n_epoch = 10\n",
    "    learning_rate = 0.001\n",
    "    conv_dropout = 0.25\n",
    "    dense_dropout = 0.5\n",
    "    \n",
    "    # Tokens\n",
    "    start_token = 2 # GO\n",
    "    end_token = 1 # PAD\n",
    "\n",
    "    # Checkpoint Path\n",
    "    ckpt_dir = 'ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.embedding_ops.embedding_lookup>"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, config, mode='training'):\n",
    "        assert mode in ['training', 'evaluation', 'inference']\n",
    "        self.mode = mode\n",
    "\n",
    "        # Model\n",
    "#         self.hidden_size = config.hidden_size\n",
    "        self.dec_emb_size = config.dec_emb_size\n",
    "        self.enc_emb_size = config.enc_emb_size\n",
    "        self.dec_sent_length = config.dec_sent_length\n",
    "        self.attn_size = config.attn_size\n",
    "        self.cell = config.cell\n",
    "        self.beam_width = config.beam_width\n",
    "        self.num_filters = config.num_filters\n",
    "        self.img_height = config.img_height\n",
    "        self.img_width = config.img_width\n",
    "        \n",
    "        # Training\n",
    "        self.optimizer = config.optimizer\n",
    "        self.n_epoch = config.n_epoch\n",
    "        self.learning_rate = config.learning_rate\n",
    "        \n",
    "        \n",
    "        # Tokens\n",
    "        self.start_token = config.start_token\n",
    "        self.end_token = config.end_token\n",
    "        \n",
    "        # Checkpoint Path\n",
    "        self.ckpt_dir = config.ckpt_dir\n",
    "        \n",
    "    def add_placeholders(self):\n",
    "\n",
    "        if self.mode == 'training':\n",
    "            self.dec_inputs = tf.placeholder(\n",
    "                tf.int32,\n",
    "                shape=[None, self.dec_sent_length+1],\n",
    "                name='target_sentences')\n",
    "\n",
    "            self.dec_sequence_length = tf.placeholder(\n",
    "                tf.int32,\n",
    "                shape=[None,],\n",
    "                name='target_sequence_length')\n",
    "        \n",
    "        self.vgg_features = tf.placeholder(tf.float32,\n",
    "                                         shape = [None, self.img_height, self.img_width, self.enc_emb_size])\n",
    "        \n",
    "        self.imbeds = tf.reshape(self.vgg_features,\n",
    "                                      shape = [-1,self.img_height*self.img_width, self.enc_emb_size])\n",
    "        \n",
    "            \n",
    "            \n",
    "    def add_decoder(self):\n",
    "        with tf.variable_scope('Decoder') as scope:\n",
    "            # get dynamic batch_size\n",
    "            batch_size = tf.shape(self.imbeds)[0]\n",
    "\n",
    "            dec_cell = self.cell(self.enc_emb_size)\n",
    "            \n",
    "            attn_mech = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                num_units=self.attn_size,\n",
    "                memory=self.imbeds,\n",
    "                name='BahdanauAttention')\n",
    "\n",
    "            dec_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell=dec_cell,\n",
    "                attention_mechanism=attn_mech,\n",
    "                name='Attention_Wrapper')\n",
    "            \n",
    "\n",
    "            # output projection (replacing `OutputProjectionWrapper`)\n",
    "            output_layer = Dense(dec_vocab_size+2, name='output_projection')\n",
    "            \n",
    "            if self.mode == 'training':\n",
    "\n",
    "                # maxium unrollings in current batch = max(dec_sent_len) + 1(GO symbol)\n",
    "                max_dec_len = tf.reduce_max(self.dec_sequence_length+1, name='max_dec_len')\n",
    "                \n",
    "                embedding_matrix = np.float32(np.eye(500))\n",
    "                \n",
    "                \n",
    "#                 dec_emb_inputs = tf.one_hot(self.dec_inputs, depth = self.dec_emb_size)\n",
    "\n",
    "                dec_emb_inputs = tf.nn.embedding_lookup(params=embedding_matrix, ids=self.dec_inputs)\n",
    "    \n",
    "                training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                    inputs=dec_emb_inputs,\n",
    "                    sequence_length=self.dec_sequence_length+1,\n",
    "                    time_major=False,\n",
    "                    name='training_helper')\n",
    "\n",
    "                training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell=dec_cell,\n",
    "                    helper=training_helper,\n",
    "                    initial_state = dec_cell.zero_state(batch_size, tf.float32),\n",
    "                    output_layer=output_layer) \n",
    "\n",
    "                train_dec_outputs, train_dec_last_state, train_dec_last_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    training_decoder,\n",
    "                    output_time_major=False,\n",
    "                    impute_finished=True,\n",
    "                    maximum_iterations=max_dec_len)\n",
    "                \n",
    "                # dec_outputs: collections.namedtuple(rnn_outputs, sample_id)\n",
    "                # dec_outputs.rnn_output: [batch_size x max(dec_sequence_len) x dec_vocab_size+2], tf.float32\n",
    "                # dec_outputs.sample_id [batch_size], tf.int32\n",
    "                \n",
    "                # logits: [batch_size x max_dec_len x dec_vocab_size+2]\n",
    "                logits = tf.identity(train_dec_outputs.rnn_output, name='logits')\n",
    "                \n",
    "                # targets: [batch_size x max_dec_len x dec_vocab_size+2]\n",
    "                targets = tf.slice(self.dec_inputs, [0, 0], [-1, max_dec_len], 'targets')\n",
    "                \n",
    "                # masks: [batch_size x max_dec_len]\n",
    "                # => ignore outputs after `dec_senquence_length+1` when calculating loss\n",
    "                masks = tf.sequence_mask(self.dec_sequence_length+1, \n",
    "                                         max_dec_len, dtype=tf.float32, name='masks')\n",
    "                \n",
    "                # Control loss dimensions with `average_across_timesteps` and `average_across_batch`\n",
    "                # internal: `tf.nn.sparse_softmax_cross_entropy_with_logits`\n",
    "                self.batch_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                    logits=logits,\n",
    "                    targets=targets,\n",
    "                    weights=masks,\n",
    "                    name='batch_loss')\n",
    "                \n",
    "                # prediction sample for validation\n",
    "                self.valid_predictions = tf.identity(train_dec_outputs.sample_id, name='valid_preds')\n",
    "\n",
    "                # List of training variables\n",
    "                self.training_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "            \n",
    "            elif self.mode == 'inference':\n",
    "            \n",
    "                start_tokens = tf.tile(tf.constant([self.start_token], dtype=tf.int32), \n",
    "                                       [batch_size], name='start_tokens')\n",
    "            \n",
    "                inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper( \n",
    "#                     embedding=self.dec_Wemb,\n",
    "                    embedding = tf.one_hot(np.arange(self.dec_emb_size),depth = self.dec_emb_size),\n",
    "                    start_tokens=start_tokens,\n",
    "                    end_token=self.end_token)\n",
    "                \n",
    "                inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell=dec_cell,\n",
    "                    helper=inference_helper,\n",
    "                    initial_state=dec_cell.zero_state(batch_size, tf.float32),\n",
    "                    output_layer=output_layer)\n",
    "                \n",
    "                infer_dec_outputs, infer_dec_last_state, infer_dec_last_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    inference_decoder,\n",
    "                    output_time_major=False,\n",
    "                    impute_finished=True,\n",
    "                    maximum_iterations=self.dec_sent_length)\n",
    "                \n",
    "                # [batch_size x dec_sentence_length], tf.int32\n",
    "                self.predictions = tf.identity(infer_dec_outputs.sample_id, name='predictions')\n",
    "                # equivalent to tf.argmax(infer_dec_outputs.rnn_output, axis=2, name='predictions')\n",
    "                \n",
    "                # List of training variables\n",
    "                self.training_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "\n",
    "    def add_training_op(self):\n",
    "        self.training_op = self.optimizer(self.learning_rate, name='training_op').minimize(self.batch_loss)\n",
    "        \n",
    "    def save(self, sess, var_list=None, save_path=None):\n",
    "        print('Saving model at {save_path}'.format(save_path=save_path))\n",
    "        if hasattr(self, 'training_variables'):\n",
    "            var_list = self.training_variables\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, save_path, write_meta_graph=False)\n",
    "        \n",
    "    def restore(self, sess, var_list=None, ckpt_path=None):\n",
    "        if hasattr(self, 'training_variables'):\n",
    "            var_list = self.training_variables\n",
    "        self.restorer = tf.train.Saver()\n",
    "#         self.restorer.restore(sess, ckpt_path)\n",
    "#tf.train.latest_checkpoint(ckpt_path)\n",
    "        self.restorer.restore(sess,ckpt_path)\n",
    "        print('Restore Finished!')\n",
    "        \n",
    "    def summary(self):\n",
    "        summary_writer = tf.summary.FileWriter(\n",
    "            logdir=self.ckpt_dir,\n",
    "            graph=tf.get_default_graph())\n",
    "        \n",
    "    def build(self):\n",
    "        self.add_placeholders()\n",
    "        self.add_decoder()\n",
    "\n",
    "    def train(self, sess, data, from_scratch=False,\n",
    "              load_ckpt=None, save_path=None):\n",
    "    \n",
    "        # Add Optimizer to current graph\n",
    "        self.add_training_op()\n",
    "        \n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        \n",
    "        # Restore Checkpoint\n",
    "        if from_scratch is False:# and os.path.isfile(load_ckpt):\n",
    "            self.restore(sess, ckpt_path=load_ckpt)\n",
    "            \n",
    "        else:\n",
    "            sess.run(init)\n",
    "        \n",
    "        input_batches, target_batches, valid_inputs, valid_targets = data\n",
    "        loss_history = []\n",
    "        valid_loss_history = []\n",
    "        \n",
    "        t = timeit.default_timer()\n",
    "        \n",
    "        \n",
    "        for epoch in range(self.n_epoch):\n",
    "\n",
    "            all_preds = []\n",
    "            all_valid_preds = []\n",
    "            epoch_loss = 0\n",
    "            valid_loss = 0\n",
    "            batch_num = 0\n",
    "        \n",
    "            for input_batch, target_batch in zip(input_batches, target_batches):\n",
    "                batch_num+=1\n",
    "                target_batch_tokens = []\n",
    "                dec_sentence_lengths = []\n",
    "\n",
    "                for target_sent in target_batch:\n",
    "                    tokens, sent_len = sent2idx(target_sent,\n",
    "                                 vocab=dec_vocab,\n",
    "                                 max_sentence_length=self.dec_sent_length)\n",
    "                    target_batch_tokens.append(tokens)\n",
    "                    dec_sentence_lengths.append(sent_len)\n",
    "       \n",
    "                # Evaluate 3 ops in the graph\n",
    "                # => valid_predictions, loss, training_op(optimzier)\n",
    "            \n",
    "                batch_preds, batch_loss, _ = sess.run(\n",
    "                    [self.valid_predictions, self.batch_loss, self.training_op],\n",
    "                    feed_dict={\n",
    "                        self.vgg_features: input_batch,\n",
    "                        self.dec_inputs: target_batch_tokens,\n",
    "                        self.dec_sequence_length: dec_sentence_lengths,\n",
    "                    })\n",
    "                loss_history.append(batch_loss)\n",
    "                epoch_loss += batch_loss\n",
    "                all_preds.append(batch_preds)\n",
    "\n",
    "                if not batch_num%25:\n",
    "                    t_ = timeit.default_timer()\n",
    "                    print(epoch, batch_num)\n",
    "                    print(t_ - t)\n",
    "                    t = t_\n",
    "                \n",
    "            loss_history.append(epoch_loss)\n",
    "            \n",
    "            if save_path:\n",
    "                print('Saving')\n",
    "                self.save(sess, save_path=save_path+'epoch_%i_attention'%(epoch))\n",
    "\n",
    "            print('Validation')\n",
    "            \n",
    "            for input_batch, target_batch in zip(valid_inputs, valid_targets):\n",
    "                target_batch_tokens = []\n",
    "                dec_sentence_lengths = []\n",
    "\n",
    "                for target_sent in target_batch:\n",
    "                    tokens, sent_len = sent2idx(target_sent,\n",
    "                                 vocab=dec_vocab,\n",
    "                                 max_sentence_length=self.dec_sent_length)\n",
    "                    target_batch_tokens.append(tokens)\n",
    "                    dec_sentence_lengths.append(sent_len)\n",
    "       \n",
    "            \n",
    "                batch_preds, batch_loss = sess.run(\n",
    "                    [self.valid_predictions, self.batch_loss],\n",
    "                    feed_dict={\n",
    "                        self.vgg_features: input_batch,\n",
    "                        self.dec_inputs: target_batch_tokens,\n",
    "                        self.dec_sequence_length: dec_sentence_lengths,\n",
    "                    })\n",
    "                valid_loss_history.append(batch_loss)\n",
    "                valid_loss += batch_loss\n",
    "                all_valid_preds.append(batch_preds)\n",
    "                \n",
    "            valid_loss_history.append(valid_loss)\n",
    "            \n",
    "            print('Epoch', epoch)\n",
    "            for input_batch, target_batch, batch_preds in zip(valid_batches[0:1], valid_targets[0:1], all_valid_preds[0:1]):\n",
    "                for input_img, target_sent, pred in zip(input_batch[:10], target_batch[:10], batch_preds[:10]):\n",
    "\n",
    "                    print('\\tPrediction:', idx2sent(pred, reverse_vocab=dec_reverse_vocab).replace(' _PAD','').replace('_GO',''))\n",
    "                    print('\\tTarget:, {target_sent}'.format(target_sent=replace_label(target_sent)))\n",
    "            print('\\tTraining loss: {epoch_loss:.2f}\\n'.format(epoch_loss=epoch_loss))\n",
    "            print('\\tValidation loss: {valid_loss:.2f}\\n'.format(valid_loss=valid_loss))\n",
    "    \n",
    "    def inference(self, sess, data, load_ckpt):\n",
    "        \n",
    "        self.restore(sess, ckpt_path=load_ckpt)\n",
    "        \n",
    "        input_batch, target_batch = data\n",
    "\n",
    "        batch_preds = []\n",
    "        batch_tokens = []\n",
    "        batch_sent_lens = []\n",
    "        \n",
    "        for target_sent in target_batch:\n",
    "            tokens, sent_len = sent2idx(target_sent,\n",
    "                         vocab=dec_vocab,\n",
    "                         max_sentence_length=self.dec_sent_length)\n",
    "            batch_tokens.append(tokens)\n",
    "            batch_sent_lens.append(sent_lens)\n",
    "        \n",
    "            \n",
    "        batch_preds, loss = sess.run(\n",
    "            [self.predictions, self.batch_loss],\n",
    "            feed_dict={\n",
    "                self.vgg_features: input_batch,\n",
    "                self.dec_inputs: batch_tokens,\n",
    "                self.dec_sequence_length: batch_sent_lens\n",
    "            })\n",
    "        \n",
    "\n",
    "        for input_img, target_sent, pred in zip(input_batch, target_batch, batch_preds):\n",
    "            print('Prediction:', idx2sent(pred, reverse_vocab=dec_reverse_vocab).replace(' _PAD','').replace('_GO',''))\n",
    "            print('Target:', target_sent, '\\n')\n",
    "            print('Validation loss', loss, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = DemoConfig()\n",
    "model = Seq2SeqModel(config, mode='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if models are correctly built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model built!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = DemoConfig()\n",
    "\n",
    "model.build()\n",
    "# model.summary()\n",
    "print('Training model built!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "config = DemoConfig()\n",
    "model = Seq2SeqModel(config, mode='inference')\n",
    "model.build()\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1244.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_imgs = [np.load(os.path.join('imbeddings2',imbed)) for imbed in x_val]\n",
    "val_tar = [y_val[i:i+batch_size] for i in range(0,len(y_val),batch_size)]\n",
    "val_inp = [val_imgs[i:i+batch_size] for i in range(0,len(x_val),batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/epoch_0_attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/epoch_0_attention\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    config = DemoConfig()\n",
    "    model = Seq2SeqModel(config, mode='training')\n",
    "    model.build()\n",
    "    data = (input_batches, target_batches, val_inp, val_tar)\n",
    "    model\n",
    "    loss_history = model.train(sess, \n",
    "                               data, \n",
    "                               from_scratch=False,\n",
    "                               save_path = 'ckpt/',\n",
    "                               load_ckpt='ckpt/epoch_0_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = Seq2SeqModel(config, mode='inference')\n",
    "    model.build()\n",
    "    for input_batch, target_batch in zip(val_inp, val_tar):\n",
    "        data = (input_batch, target_batch)\n",
    "        model.infer(sess, data, load_ckpt='ckptepoch_2_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
