{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Figure out how to deal with epochs\n",
    "- Ensemble - figure out how to do this\n",
    "- Checkpoints\n",
    "- How to involve test-set\n",
    "- Early-stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[03/06/2017] I have made the following changes to my code:\n",
    "- Added steps to save to and restore from a checkpoint\n",
    "- Added a global step that keeps track of how many iterations have been run in total (even if the model is paused and restored)\n",
    "\n",
    "I now need to:\n",
    "- Add global step to the graph\n",
    "- Figure out how to deal with epochs\n",
    "- Figure out how which checkpoint is restored - it seems that by default the file 'checkpoint' is the latest version. Also by default all variables are stored (this is recommended it seems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 2048\n",
    "N_FEATURES = 30\n",
    "TRAIN_SIZE = 727722\n",
    "VAL_SIZE = 80858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_lstm = np.random.randint(175,275)\n",
    "num_dense = np.random.randint(100,150)\n",
    "rate_drop_lstm = 0.15+np.random.rand()*0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand()*0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('embeddings.pkl','rb') as f:\n",
    "    embedding_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act = 'relu'\n",
    "re_weight = True\n",
    "nb_words = embedding_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_queues(filename,batch_size=BATCH_SIZE):\n",
    "    train_q = tf.train.string_input_producer([filename])\n",
    "    \n",
    "    \n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    _, value = reader.read(train_q)\n",
    "\n",
    "    record_defaults = [[1] for i in range(2*N_FEATURES+3)]\n",
    "\n",
    "    content = tf.decode_csv(value,record_defaults=record_defaults)\n",
    "\n",
    "    seq = tf.stack(content[:2*N_FEATURES])\n",
    "\n",
    "    length = tf.stack(content[2*N_FEATURES:-1])\n",
    "    \n",
    "    label = content[-1]\n",
    "\n",
    "    min_after_dequeue = 10*batch_size\n",
    "\n",
    "    capacity = 20*batch_size\n",
    "\n",
    "    q = tf.RandomShuffleQueue(\n",
    "            dtypes=[seq.dtype, length.dtype, label.dtype],\n",
    "            shapes = [seq.shape, length.shape, label.shape],\n",
    "            capacity=capacity,\n",
    "            min_after_dequeue=min_after_dequeue\n",
    "            )\n",
    "\n",
    "    enqueue_op = q.enqueue([seq, length, label])\n",
    "    numberOfThreads = 1\n",
    "    qr = tf.train.QueueRunner(q,[enqueue_op]*numberOfThreads)\n",
    "    tf.train.add_queue_runner(qr)\n",
    "    \n",
    "    return q\n",
    "\n",
    "def batch_generator(batch_size):\n",
    "    select_q = tf.placeholder(tf.int32,name='select_q')\n",
    "    \n",
    "    train_q,val_q = [build_queues('seqs_%s.csv'%name,batch_size) \n",
    "                     for name in ['train','val']] \n",
    "    q = tf.QueueBase.from_list(select_q, [train_q, val_q])\n",
    "    \n",
    "    data_batch, length_batch, label_batch = q.dequeue_many(batch_size)\n",
    "    return data_batch, length_batch, label_batch, select_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_input(test_size):\n",
    "    train_q = tf.train.string_input_producer(['test'])\n",
    "    \n",
    "    \n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    _, value = reader.read_up_to(train_q,test_size)\n",
    "\n",
    "    record_defaults = [[1] for i in range(2*N_FEATURES+3)]\n",
    "\n",
    "    content = tf.decode_csv(value,record_defaults=record_defaults)\n",
    "\n",
    "    seq = tf.stack(content[:2*N_FEATURES])\n",
    "\n",
    "    length = tf.stack(content[2*N_FEATURES:-1])\n",
    "    \n",
    "    label = content[-1]\n",
    "    \n",
    "    return seq, length, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_normalize(X,is_tr,eps=1e-5,momentum=0.9):\n",
    "    moments_shape = (1,X.get_shape()[-1].value)\n",
    "    running_mean = tf.get_variable('rm',shape=moments_shape,trainable=False,\n",
    "                                   initializer=tf.zeros_initializer())\n",
    "    running_var = tf.get_variable('rv',shape=moments_shape,trainable=False,\n",
    "                                  initializer=tf.ones_initializer())\n",
    "    \n",
    "    mu,var = tf.nn.moments(X,axes=[0],keep_dims=True)\n",
    "\n",
    "    gamma = tf.get_variable('gamma',shape=moments_shape)\n",
    "    beta = tf.get_variable('beta',shape=moments_shape)\n",
    "    \n",
    "    bn = tf.nn.batch_normalization(\n",
    "            X,\n",
    "            tf.cond(is_tr,lambda:mu,lambda:running_mean),\n",
    "            tf.cond(is_tr,lambda:var,lambda:running_var),\n",
    "            gamma,\n",
    "            beta,\n",
    "            eps\n",
    "        )\n",
    "    \n",
    "    tf.cond(is_tr,lambda:update_moments(running_mean,running_var,mu,var,momentum),tf.no_op)\n",
    "    \n",
    "    return bn\n",
    "\n",
    "def update_moments(running_mean,running_var,mu,var,momentum):\n",
    "    old_weight = momentum\n",
    "    new_weight = 1-momentum\n",
    "    tf.add_to_collection(\n",
    "        tf.assign(running_mean,old_weight*running_mean + new_weight*mu),\n",
    "        tf.GraphKeys.UPDATE_OPS\n",
    "        )\n",
    "    tf.add_to_collection(\n",
    "        tf.assign(running_var, old_weight*running_var + new_weight*var),\n",
    "        tf.GraphKeys.UPDATE_OPS\n",
    "        )\n",
    "    return tf.no_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_graph(\n",
    "    vocab_size = nb_words,\n",
    "    state_size = num_lstm,\n",
    "    embed_size = EMBEDDING_DIM,\n",
    "    fc_size = num_dense,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_classes = 2,\n",
    "    lr = 1e-4,\n",
    "):\n",
    "    reset_graph()\n",
    "    \n",
    "    is_tr = tf.placeholder(tf.bool)\n",
    "    is_te = tf.placeholder(tf.bool)\n",
    "    select_q = tf.placeholder(tf.int32)\n",
    "    \n",
    "    \n",
    "    data_batch, length_batch, y = \\\n",
    "    tf.cond(\n",
    "            is_test,\n",
    "            get_test_input(test_size),\n",
    "            batch_generator(batch_size)\n",
    "        )\n",
    "    \n",
    "    X,seqlens = (tf.concat(tf.split(orig,2,axis=1),axis=0) for orig in \n",
    "                   [data_batch, length_batch])\n",
    "    \n",
    "    seqlens = tf.reshape(seqlens,[-1])\n",
    "    \n",
    "    rate_drop_dense = tf.placeholder(tf.float32)\n",
    "    rate_drop_lstm = tf.placeholder(tf.float32)\n",
    "    \n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "    \n",
    "    with tf.variable_scope('embed'):\n",
    "    \n",
    "        embeddings = tf.get_variable('embeddings',\n",
    "                                    (vocab_size, embed_size),\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=False)\n",
    "        rnn_inputs = tf.nn.embedding_lookup(embeddings, X)\n",
    "    \n",
    "    with tf.variable_scope('lstm'):\n",
    "        cell = tf.contrib.rnn.LSTMCell(state_size)\n",
    "        rnn_outputs, final_state = tf.nn.dynamic_rnn(\n",
    "                                                     cell=cell,\n",
    "                                                     inputs=rnn_inputs,\n",
    "                                                     dtype=tf.float32,\n",
    "                                                     sequence_length = seqlens)\n",
    "        \n",
    "        rnn_outputs = tf.concat(rnn_outputs, 2)\n",
    "        rnn_outputs = tf.nn.dropout(rnn_outputs, rate_drop_dense)\n",
    "        range_size = tf.cond(is_test,tf.constant(test_size),tf.constant(batch_size))\n",
    "        idx = tf.range(2*range_size)*tf.shape(rnn_outputs)[1]\\\n",
    "                + (seqlens - 1)\n",
    "        last_rnn_outputs = tf.gather(tf.reshape(rnn_outputs,\n",
    "                                               [-1, state_size]), idx)\n",
    "        split = tf.split(last_rnn_outputs,2)\n",
    "        merged = tf.concat(split,axis=1)\n",
    "    \n",
    "    with tf.variable_scope('bn1'):\n",
    "        merged = batch_normalize(merged,is_tr)\n",
    "    \n",
    "    with tf.variable_scope('fc1'):\n",
    "        colms = X.get_shape()[-1].value\n",
    "        W1 = tf.get_variable('W1',shape=[2*state_size,num_dense])\n",
    "        b1 = tf.get_variable('b1',shape=[num_dense])\n",
    "\n",
    "        merged = tf.nn.xw_plus_b(merged,W1,b1)\n",
    "        merged = tf.nn.dropout(merged,rate_drop_dense)\n",
    "    \n",
    "    with tf.variable_scope('bn2'):\n",
    "        merged = batch_normalize(merged,is_tr)\n",
    "        \n",
    "    with tf.variable_scope('softmax'):\n",
    "        W_out = tf.get_variable('W_out', [num_dense, 2])\n",
    "        b_out = tf.get_variable('b_out', [2],\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "        logits = tf.nn.xw_plus_b(merged,W_out,b_out)\n",
    "        \n",
    "        preds = tf.nn.softmax(logits)\n",
    "        correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32),y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "        )\n",
    "    \n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "         train_step = tf.train.AdamOptimizer(lr).minimize(loss,\n",
    "                                                          global_step=global_step)\n",
    "    \n",
    "    return {\n",
    "        'select_q':select_q,\n",
    "        'rate_drop_dense' : rate_drop_dense,\n",
    "        'rate_drop_lstm' : rate_drop_lstm,\n",
    "        'loss' : loss,\n",
    "        'ts' : train_step,\n",
    "        'preds' : preds,\n",
    "        'accuracy' : accuracy,\n",
    "        'is_train':is_tr,\n",
    "        'embeddings':embeddings,\n",
    "        'global_step':global_step,\n",
    "        'y':y\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(sess,fetch,feed,batches_per_epoch):\n",
    "    acc,loss = 0,0\n",
    "    for step in range(batches_per_epoch):\n",
    "        acc_,loss_ = sess.run(fetch, feed_dict=feed)\n",
    "        acc+=acc_\n",
    "        loss+=loss_\n",
    "\n",
    "    return acc/batches_per_epoch, loss/batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_graph(graph,\n",
    "                filename,\n",
    "                dir_,\n",
    "                train_size = TRAIN_SIZE,\n",
    "                val_size = VAL_SIZE,\n",
    "                batch_size = BATCH_SIZE,\n",
    "                num_epochs = 10, \n",
    "                loss_every=50):\n",
    "    \n",
    "    batches_per_epoch = {'train':int(np.round(train_size/batch_size)),\n",
    "                         'test':int(np.round(val_size/batch_size))}\n",
    "    \n",
    "    g = graph\n",
    "    \n",
    "    path = 'checkpoints/'+ dir_+'/%s'\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "#         saver = tf.train.Saver()\n",
    "#         ckpt = tf.train.get_checkpoint_state(os.path.dirname(path%'checkpoint'))\n",
    "#         if ckpt and ckpt.model_checkpoint_path:\n",
    "#             saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                \n",
    "        initial_step = g['global_step'].eval()\n",
    "        \n",
    "        print(initial_step)\n",
    "        \n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        step, accuracy,loss = initial_step, 0, 0\n",
    "        \n",
    "        tr_accs, te_accs = [], []\n",
    "        tr_losses, te_losses = [], []\n",
    "        \n",
    "        current_epoch = 0\n",
    "        \n",
    "        skip_step = 0\n",
    "        \n",
    "        for step in range(initial_step+1,batches_per_epoch['train']*num_epochs+1):\n",
    "            #If restarting at a later point want to ensure that\n",
    "            #accumulated accuracies and losses are averaged properly\n",
    "            #Maybe it would be better somehow to save the accumulated loss but we will stick\n",
    "            #with this for the moment \n",
    "            rel_step = step-initial_step \n",
    "            skip_step += 1\n",
    "            \n",
    "            feed = {g['select_q']:0,\n",
    "                    g['is_train']:True, \n",
    "                    g['embeddings']:embedding_matrix,\n",
    "                    g['rate_drop_dense']:rate_drop_dense,\n",
    "                    g['rate_drop_lstm']:rate_drop_lstm }\n",
    "            accuracy_, _,loss_ = sess.run([g['accuracy'], \n",
    "                                           g['ts'],g['loss']], \n",
    "                                         feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "            loss += loss_\n",
    "            \n",
    "            if (rel_step%loss_every) == 0: \n",
    "                step_string= '%d step%s'%(int(step),'s'*(int(step)>1))\n",
    "                print('Average loss at %s = %f'%(step_string,loss / skip_step))\n",
    "#                 saver.save(sess, path%filename, step)\n",
    "            \n",
    "            \n",
    "            if (rel_step%batches_per_epoch['train']) == 0:\n",
    "                current_epoch += 1\n",
    "                tr_accs.append(accuracy / rel_step)\n",
    "                tr_losses.append(loss / rel_step)\n",
    "                loss, accuracy = 0, 0\n",
    "                skip_step = 0\n",
    "                \n",
    "                fetch = [g['accuracy'],g['loss']]\n",
    "                feed = {g['select_q']:1, \n",
    "                        g['is_train']:True, \n",
    "                        g['embeddings']:embedding_matrix,\n",
    "                        g['rate_drop_dense']:rate_drop_dense,\n",
    "                        g['rate_drop_lstm']:rate_drop_lstm }\n",
    "\n",
    "                avg_val_acc, avg_val_loss = evaluate(sess,fetch,feed,batches_per_epoch['test'])\n",
    "                te_accs.append(avg_val_acc)\n",
    "                te_losses.append(avg_val_loss)\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", \n",
    "                      tr_accs[-1], \"- te:\", te_accs[-1])\n",
    "                \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        \n",
    "    return tr_losses, te_losses, tr_accs, te_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'seqs_train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-abe467b0281c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seqs_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'seqs_train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "len(pd.read_csv('seqs_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/lstm_v/lstm-700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/lstm_v/lstm-700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, seqs_train.csv\n",
      "\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TextLineReaderV2, input_producer)]]\n",
      "\n",
      "Caused by op 'ReaderReadV2', defined at:\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 405, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/ioloop.py\", line 827, in start\n",
      "    self._run_callback(callback)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/ioloop.py\", line 600, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 242, in enter_eventloop\n",
      "    self.eventloop(self)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/eventloops.py\", line 241, in loop_cocoa\n",
      "    show.mainloop()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/matplotlib/backends/backend_macosx.py\", line 26, in mainloop\n",
      "    _macosx.show()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/matplotlib/backend_bases.py\", line 1311, in _on_timer\n",
      "    ret = func(*args, **kwargs)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/eventloops.py\", line 218, in doi\n",
      "    kernel.do_one_iteration()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 275, in do_one_iteration\n",
      "    stream.flush(zmq.POLLIN, 1)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 352, in flush\n",
      "    self._handle_recv()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-15-cf9f68c3d8b9>\", line 1, in <module>\n",
      "    graph = build_graph()\n",
      "  File \"<ipython-input-11-6fea49f57213>\", line 14, in build_graph\n",
      "    data_batch, length_batch, y, select_q = batch_generator(batch_size)\n",
      "  File \"<ipython-input-8-dde9e1a413e0>\", line 40, in batch_generator\n",
      "    for name in ['train','val']]\n",
      "  File \"<ipython-input-8-dde9e1a413e0>\", line 40, in <listcomp>\n",
      "    for name in ['train','val']]\n",
      "  File \"<ipython-input-8-dde9e1a413e0>\", line 6, in build_queues\n",
      "    _, value = reader.read(train_q)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/ops/io_ops.py\", line 193, in read\n",
      "    return gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 411, in _reader_read_v2\n",
      "    queue_handle=queue_handle, name=name)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n",
      "    op_def=op_def)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n",
      "    original_op=self._default_original_op, op_def=op_def)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n",
      "    self._traceback = _extract_stack()\n",
      "\n",
      "NotFoundError (see above for traceback): seqs_train.csv\n",
      "\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TextLineReaderV2, input_producer)]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, seqs_train.csv\n",
      "\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TextLineReaderV2, input_producer)]]\n",
      "\n",
      "Caused by op 'ReaderReadV2', defined at:\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 405, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/ioloop.py\", line 827, in start\n",
      "    self._run_callback(callback)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/ioloop.py\", line 600, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 242, in enter_eventloop\n",
      "    self.eventloop(self)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/eventloops.py\", line 241, in loop_cocoa\n",
      "    show.mainloop()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/matplotlib/backends/backend_macosx.py\", line 26, in mainloop\n",
      "    _macosx.show()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/matplotlib/backend_bases.py\", line 1311, in _on_timer\n",
      "    ret = func(*args, **kwargs)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/eventloops.py\", line 218, in doi\n",
      "    kernel.do_one_iteration()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 275, in do_one_iteration\n",
      "    stream.flush(zmq.POLLIN, 1)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 352, in flush\n",
      "    self._handle_recv()\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-15-cf9f68c3d8b9>\", line 1, in <module>\n",
      "    graph = build_graph()\n",
      "  File \"<ipython-input-11-6fea49f57213>\", line 14, in build_graph\n",
      "    data_batch, length_batch, y, select_q = batch_generator(batch_size)\n",
      "  File \"<ipython-input-8-dde9e1a413e0>\", line 40, in batch_generator\n",
      "    for name in ['train','val']]\n",
      "  File \"<ipython-input-8-dde9e1a413e0>\", line 40, in <listcomp>\n",
      "    for name in ['train','val']]\n",
      "  File \"<ipython-input-8-dde9e1a413e0>\", line 6, in build_queues\n",
      "    _, value = reader.read(train_q)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/ops/io_ops.py\", line 193, in read\n",
      "    return gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 411, in _reader_read_v2\n",
      "    queue_handle=queue_handle, name=name)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n",
      "    op_def=op_def)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n",
      "    original_op=self._default_original_op, op_def=op_def)\n",
      "  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n",
      "    self._traceback = _extract_stack()\n",
      "\n",
      "NotFoundError (see above for traceback): seqs_train.csv\n",
      "\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TextLineReaderV2, input_producer)]]\n",
      "\n"
     ]
    },
    {
     "ename": "OutOfRangeError",
     "evalue": "RandomShuffleQueue '_13_random_shuffle_queue' is closed and has insufficient elements (requested 2048, current size 0)\n\t [[Node: Gather_DequeueMany = QueueDequeueManyV2[component_types=[DT_INT32, DT_INT32, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Gather, Gather_DequeueMany/n)]]\n\nCaused by op 'Gather_DequeueMany', defined at:\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 405, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/ioloop.py\", line 827, in start\n    self._run_callback(callback)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/ioloop.py\", line 600, in _run_callback\n    ret = callback()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 242, in enter_eventloop\n    self.eventloop(self)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/eventloops.py\", line 241, in loop_cocoa\n    show.mainloop()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/matplotlib/backends/backend_macosx.py\", line 26, in mainloop\n    _macosx.show()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/matplotlib/backend_bases.py\", line 1311, in _on_timer\n    ret = func(*args, **kwargs)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/eventloops.py\", line 218, in doi\n    kernel.do_one_iteration()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 275, in do_one_iteration\n    stream.flush(zmq.POLLIN, 1)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 352, in flush\n    self._handle_recv()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-cf9f68c3d8b9>\", line 1, in <module>\n    graph = build_graph()\n  File \"<ipython-input-11-6fea49f57213>\", line 14, in build_graph\n    data_batch, length_batch, y, select_q = batch_generator(batch_size)\n  File \"<ipython-input-8-dde9e1a413e0>\", line 43, in batch_generator\n    data_batch, length_batch, label_batch = q.dequeue_many(batch_size)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 458, in dequeue_many\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1328, in _queue_dequeue_many_v2\n    timeout_ms=timeout_ms, name=name)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nOutOfRangeError (see above for traceback): RandomShuffleQueue '_13_random_shuffle_queue' is closed and has insufficient elements (requested 2048, current size 0)\n\t [[Node: Gather_DequeueMany = QueueDequeueManyV2[component_types=[DT_INT32, DT_INT32, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Gather, Gather_DequeueMany/n)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: RandomShuffleQueue '_13_random_shuffle_queue' is closed and has insufficient elements (requested 2048, current size 0)\n\t [[Node: Gather_DequeueMany = QueueDequeueManyV2[component_types=[DT_INT32, DT_INT32, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Gather, Gather_DequeueMany/n)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6f0b462817d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m tr_losses, te_losses, tr_accs, te_accs = train_graph(graph,'lstm','lstm_v',\n\u001b[1;32m      4\u001b[0m                                                      \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                      loss_every=50)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-22ce16308793>\u001b[0m in \u001b[0;36mtrain_graph\u001b[0;34m(graph, filename, dir_, train_size, val_size, batch_size, num_epochs, loss_every)\u001b[0m\n\u001b[1;32m     51\u001b[0m             accuracy_, _,loss_ = sess.run([g['accuracy'], \n\u001b[1;32m     52\u001b[0m                                            g['ts'],g['loss']], \n\u001b[0;32m---> 53\u001b[0;31m                                          feed_dict=feed)\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: RandomShuffleQueue '_13_random_shuffle_queue' is closed and has insufficient elements (requested 2048, current size 0)\n\t [[Node: Gather_DequeueMany = QueueDequeueManyV2[component_types=[DT_INT32, DT_INT32, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Gather, Gather_DequeueMany/n)]]\n\nCaused by op 'Gather_DequeueMany', defined at:\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 405, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/ioloop.py\", line 827, in start\n    self._run_callback(callback)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/ioloop.py\", line 600, in _run_callback\n    ret = callback()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 242, in enter_eventloop\n    self.eventloop(self)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/eventloops.py\", line 241, in loop_cocoa\n    show.mainloop()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/matplotlib/backends/backend_macosx.py\", line 26, in mainloop\n    _macosx.show()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/matplotlib/backend_bases.py\", line 1311, in _on_timer\n    ret = func(*args, **kwargs)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/eventloops.py\", line 218, in doi\n    kernel.do_one_iteration()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 275, in do_one_iteration\n    stream.flush(zmq.POLLIN, 1)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 352, in flush\n    self._handle_recv()\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-cf9f68c3d8b9>\", line 1, in <module>\n    graph = build_graph()\n  File \"<ipython-input-11-6fea49f57213>\", line 14, in build_graph\n    data_batch, length_batch, y, select_q = batch_generator(batch_size)\n  File \"<ipython-input-8-dde9e1a413e0>\", line 43, in batch_generator\n    data_batch, length_batch, label_batch = q.dequeue_many(batch_size)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 458, in dequeue_many\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1328, in _queue_dequeue_many_v2\n    timeout_ms=timeout_ms, name=name)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/user/anaconda/envs/tensorflow3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nOutOfRangeError (see above for traceback): RandomShuffleQueue '_13_random_shuffle_queue' is closed and has insufficient elements (requested 2048, current size 0)\n\t [[Node: Gather_DequeueMany = QueueDequeueManyV2[component_types=[DT_INT32, DT_INT32, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Gather, Gather_DequeueMany/n)]]\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "tr_losses, te_losses, tr_accs, te_accs = train_graph(graph,'lstm','lstm_v',\n",
    "                                                     num_epochs = 10,\n",
    "                                                     loss_every=50)\n",
    "print(timeit.default_timer()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
